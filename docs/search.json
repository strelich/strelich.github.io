[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Statistical Methods lecture videos\n\n\n\n\n\n\nTeaching\n\n\n\nLecture videos from my Statistical Methods course now on Youtube\n\n\n\n\n\nFeb 13, 2025\n\n\nJack Strelich\n\n\n\n\n\n\n\n\n\n\n\n\nIMDB Data Wrangling\n\n\n\n\n\n\nR\n\n\nData Cleaning\n\n\nData Visualization\n\n\nAnalysis\n\n\n\nWrangling IMBD data and modelling with tidymodels\n\n\n\n\n\nJan 23, 2025\n\n\nJack Strelich\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization with ggplot2\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\n\nAn introduction to data visualization with ggplot2\n\n\n\n\n\nNov 25, 2024\n\n\nJack Strelich\n\n\n\n\n\n\n\n\n\n\n\n\nNYC Crash Data\n\n\n\n\n\n\nR\n\n\nData Cleaning\n\n\nData Visualization\n\n\nMapping\n\n\n\nExploring a data set of motor vehicle crashes in New York City\n\n\n\n\n\nMay 6, 2024\n\n\nJack Strelich\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "I’m Jack Strelich, and this is my website! I blog about data wrangling, analysis, and visualization."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Welcome",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\nStatistical Methods lecture videos\n\n\nLecture videos from my Statistical Methods course now on Youtube\n\n\n\nJack Strelich\n\n\nFeb 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIMDB Data Wrangling\n\n\nWrangling IMBD data and modelling with tidymodels\n\n\n\nJack Strelich\n\n\nJan 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization with ggplot2\n\n\nAn introduction to data visualization with ggplot2\n\n\n\nJack Strelich\n\n\nNov 25, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n See all"
  },
  {
    "objectID": "posts/2025-02-13-Youtube-stats-lectures/index.html",
    "href": "posts/2025-02-13-Youtube-stats-lectures/index.html",
    "title": "Statistical Methods lecture videos",
    "section": "",
    "text": "I have a YouTube! I’ve started uploading lecture videos from the Statistical Methods in Psychological Science course I taught in Summer 2020. The production is a little rough around the edges, but overall I think they’re a good representation of my teaching in an academic setting – check ’em out!\nA little background: in December 2019, I signed up to teach two courses (Statistical Methods and Developmental Psychology) the following summer, my first courses as instructor of record. Then, of course, everything changed when the Fire Nation COVID-19 attacked and the university hastily transitioned to remote instruction. As I designed my courses, I chose to emphasize flexibility1 for the students: replacing the conventional high-stakes midterm and final exam with weekly assessments, dropping the lowest score on exams and homework assignments, and replacing lectures with asynchronous (pre-recorded) lecture videos.\nBecause the lecture videos would be the “face” of the class (especially for the students who didn’t attend office hours), I invested time in the production process, recording multiple takes of each slide and splicing out garbled lines to ensure that my narration flowed smoothly. Producing the videos also served as a crash course in audio engineering; I borrowed a Blue Yeti microphone from a friend to replace my laptop microphone and set up an ersatz recording studio, with a pop shield in front of the mic, a folding screen behind me and throw pillows propped in front of me to reduce reverb, and a mousepad under the mic to reduce keyboard noise.\nThe production process was time-consuming: an hour-long lecture took about two hours of recording, another hour of editing, and roughly half an hour to upload to Panopto. Still, because the lecture videos were the main point of contact with the course material for my students, I felt that this was time well-spent. Both in office hours and in anonymous feedback, students said that they enjoyed the flexibility of the asynchronous lectures, and that they found the information clear and easy to follow.\nI currently have the first two weeks (out of six) posted to a playlist on my YouTube. While there are some things I’d do differently if I were to teach this course again (e.g., a full-on flipped classroom approach, and better audio mixing), I’m still very proud of the end product. Let me know what you think!"
  },
  {
    "objectID": "posts/2025-02-13-Youtube-stats-lectures/index.html#footnotes",
    "href": "posts/2025-02-13-Youtube-stats-lectures/index.html#footnotes",
    "title": "Statistical Methods lecture videos",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAt this stage, I was mainly concerned about disruptions due to COVID-19; over the course of the summer, my students would also experience disruption due to civil unrest, natural disasters, and other forces majeure. It was a very interesting time to be alive!↩︎"
  },
  {
<<<<<<< HEAD
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html",
    "href": "posts/2024-05-04-NYC-Crashes/index.html",
    "title": "NYC Crash Data",
    "section": "",
    "text": "In this post, I’ll explore a data set of motor vehicle crashes in New York City obtained via data.gov. This data set interested me for two reasons. First, it’s quite comprehensive, covering all crashes resulting in injury and/or over $1000 in damage since mid-2012. Second, it’s messy in a way that’s very representative of real-world data (and very hard to replicate with more “shrink-wrapped” data sets).\nMy goals for this analysis were to perform some basic data cleaning, get some insights from the data, and practice a type of visualization I don’t get to use very often in my day job: the choropleth map!\nTo start, we’ll load the packages we need:\n\n# Load packages\n## Handling dates\nlibrary(lubridate)\n\n## Mapping\nlibrary(sf)\nlibrary(nycgeo)\n\n## And starring...\nlibrary(tidyverse)\n\n# Set theme for plots\ntheme_set(theme_minimal())\n\nNow we’ll read in the data. Because the CSV is so large (~437 Mb), I decided to read it directly via the URL, rather than downloading and reading the file1. Once the data is loaded, we’ll tweak the column names for readability (converting to sentence case and replacing spaces with underscores), read the Crash_date column via lubridate::as_date(), and change the names of the columns containing vehicle type codes to be consistent with other column names. We’ll also filter the data to look only at crashes in 2023, to avoid using too much memory2.\n\n# Load crash data (https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv)\ncrash_url &lt;- \"https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv\"\n\n# Load data (takes a minute!)\ncrash_dat &lt;- vroom::vroom(file = crash_url, # Read from URL\n                          guess_max = 10^3,\n                          .name_repair = ~str_replace_all(str_to_sentence(.x), \n                                                          pattern = \" \", \n                                                          replacement = \"_\")) %&gt;%  \n  mutate(Crash_date= as_date(Crash_date, format = \"%m/%d/%Y\")) %&gt;% \n  filter(year(Crash_date) == 2023) %&gt;% # Only 2023 data\n  rename_with(\\(var) str_replace_all(var, \n                                     pattern = \"Vehicle_type_code\", \n                                     replacement = \"Type_code_vehicle\"))\n\nRows: 2084770 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (16): Crash_date, Borough, Location, On_street_name, Cross_street_name,...\ndbl  (12): Zip_code, Latitude, Longitude, Number_of_persons_injured, Number_...\ntime  (1): Crash_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#time-and-date",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#time-and-date",
    "title": "NYC Crash Data",
    "section": "Time and date",
    "text": "Time and date\nFirst, let’s look at incidents by month:\n\nggplot(crash_dat, aes(x = month(Crash_date, label = TRUE))) + \n  geom_bar() +\n  labs(x = \"Month\", y = \"Incidents\")\n\n\n\n\n\n\n\nFigure 1: Incidents by date\n\n\n\n\n\nNo obvious differences by month/season! Next, let’s consider crash times. Looking at raw crash time data, there seems to be an unusually high number of crashes at exactly midnight (00:00):\n\ncount(crash_dat, Crash_time,sort = TRUE) %&gt;% head(n=10)\n\n\n  \n\n\n\nIn fact, we see almost twice as many crashes reported at midnight as at 5pm! This makes me suspect that in at least some cases a crash time of 00:00 represents missing data; for the purposes of this graph, we’ll exclude them.\nNow to see whether the time that incidents occurred varied by month. To help us compare months, we’ll use a ridgeline plot3, via ggridges::geom_density_ridges():\n\ncrash_dat %&gt;% \n  filter(Crash_time &gt; 0) %&gt;% \n  mutate(Crash_month = month(Crash_date, label = TRUE)) %&gt;% \n  ggplot(aes(x=Crash_time, \n             color=Crash_month,\n             fill=Crash_month,\n             y = fct_rev(Crash_month))) + \n  ggridges::geom_density_ridges(alpha = 0.4,\n                                quantile_lines = TRUE,\n                                quantiles = 2) +\n  scale_color_viridis_d() +\n  scale_x_time(breaks = (0:6)*14400) +\n  labs(x = \"Time of day\", y = \"Month\", \n       caption = \"Vertical lines represent median.\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 2: Incidents by month and time of day\n\n\n\n\n\nCrashes by time of day look fairly consistent across months; notice that the median of each distribution is around 2pm. Additionally, we can see peaks in the number of incidents between 8-9am and between 5-6pm (representing rush hour, I’m assuming)."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#injuries-and-fatalities",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#injuries-and-fatalities",
    "title": "NYC Crash Data",
    "section": "Injuries and fatalities",
    "text": "Injuries and fatalities\nTo explore crash outcomes, we’ll use pivot_longer() to make a data frame with multiple rows per crash, each row representing the number of motorists, cyclists, or pedestrians (Type) who were injured or killed (Outcome) in that crash.\n\noutcomes_by_crash &lt;- crash_dat %&gt;% \n  select(Collision_id, \n         starts_with(\"Crash\"), # Keep date and time\n         starts_with(\"Number_of_\")) %&gt;%\n  pivot_longer(cols = starts_with(\"Number_of_\"), \n               names_prefix = \"Number_of_\", # Clean up col names\n               names_sep = \"\\\\_\", # Split at underscore\n               names_to = c(\"Type\",\"Outcome\")) %&gt;% \n  mutate(Crash_month = month(Crash_date, label = TRUE),\n         Type = str_to_title(Type),\n         Outcome = str_to_title(Outcome))\n\n# Check our work\nhead(outcomes_by_crash, n = 10)\n\n\n  \n\n\n\nNow we can plot total injuries/fatalities for motorists, cyclists, and pedestrians by month. Because fatalities are (thankfully) far rarer than injuries, we’ll use the scales = \"free_y\" argument to facet_wrap() to let the two halves of the plot use different y-axis limits. We’ll also use fct_reorder() to make sure the ordering of categories in the legend matches the ordering of the categories in the graph itself.\n\noutcomes_by_crash %&gt;% \n  filter(Type != \"Persons\") %&gt;% \n  mutate(Type = fct_reorder(Type, value, sum, .desc = T)) %&gt;% \n  ggplot(aes(x = Crash_month, \n             y = value, \n             color = Type,\n             group = Type)) + \n  stat_summary(geom=\"line\", fun = \"sum\") +\n  stat_summary(geom=\"point\", fun = \"sum\") + \n  facet_wrap(~Outcome,\n             scales = \"free_y\",\n             nrow = 2) + \n  labs(y = \"Number of persons\", x= \"Month\")\n\n\n\n\n\n\n\nFigure 3: Injuries and fatalities by month\n\n\n\n\n\nInterestingly, injuries and fatalities for motorists seem to be highest in summer, while injuries and fatalities for pedestrians are lower during these months."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#vehicles",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#vehicles",
    "title": "NYC Crash Data",
    "section": "Vehicles",
    "text": "Vehicles\nNext, let’s explore the contributing factors and types of vehicles involved.\n\nCleaning\nWe’ll restructure the data such that each row represents one vehicle, rather than one incident!\n\nvehicle_dat &lt;- crash_dat %&gt;% \n  select(Collision_id, starts_with(c(\"Contributing_factor\",\"Type_code\"))) %&gt;% \n  pivot_longer(-Collision_id, \n               names_sep = \"_vehicle_\", \n               names_to = c(\".value\",\"Vehicle\")) %&gt;% \n  drop_na() %&gt;% \n  mutate(Vehicle = as.factor(as.numeric(Vehicle)))\n\nNow we can look at the different vehicle type codes and contributing factors that occur in the data:\n\ncount(vehicle_dat, Type_code, sort = TRUE)\ncount(vehicle_dat, Contributing_factor, sort = TRUE)\n\n\n\nTable 1: Incidents by vehicle type and contributing factor (raw)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\nThere are a lot of categories in here, and many of them appear to overlap (e.g., “Motorcycle” and “Motorbike”). To clean up a bit, we’ll change all Type_code entries to sentence case, then manually consolidate some levels:\n\nvehicle_dat &lt;- vehicle_dat %&gt;% \n  mutate(Type_code = str_to_sentence(Type_code),\n         Type_code = case_when( # checks a series of conditionals\n           str_detect(Type_code, \n                      coll(\"wagon\", ignore_case = TRUE)) ~ \"SUV/station wagon\",\n           str_detect(Type_code, \n                      coll(\"sedan\",ignore_case = TRUE)) ~ \"Sedan\",\n           .default = Type_code),\n         Type_code = case_match( # Replaces (vectors of) matches, OLD ~ NEW\n           Type_code,\n           \"Bicycle\" ~ \"Bike\",\n           \"Motorbike\" ~ \"Motorcycle\",\n           c(\"Ambul\",\"Ambu\",\"Amb\") ~ \"Ambulance\",\n           c(\"Unkno\",\"Unk\") ~ \"Unknown\",\n           c(\"Fire\",\"Fdny\",\"Firetruck\",\n             \"Firet\",\"Fdny truck\",\"Fdny fire\") ~ \"Fire truck\",\n           \"E-sco\" ~ \"E-scooter\",\n           \"E-bik\" ~ \"E-bike\",\n           .default = Type_code) %&gt;% \n           fct_lump_prop(0.005), # Lump codes occurring less than 0.5% \n         Contributing_factor = fct_lump_prop(\n           str_to_sentence(Contributing_factor), \n           0.005)) # same for contributing factor\n\nNow let’s check our work:\n\n(crashes_by_type &lt;- count(vehicle_dat, Type_code, \n                          sort = TRUE, name = \"Crashes\"))\n(crashes_by_factor &lt;- count(vehicle_dat, Contributing_factor, \n                            sort = TRUE, name = \"Crashes\"))\n\n\n\nTable 2: Incidents by vehicle type and contributing factor\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\nIt’s not perfect, and a subject matter expert could probably identify more types/factors that could be combined, but it should be workable for our purposes.\n\n\nExploring\n\nWhat types of vehicles were involved in crashes?\n\nggplot(crashes_by_type, aes(x = fct_rev(fct_inorder(Type_code)), y = Crashes)) + \n  geom_col() + coord_flip() +\n  labs(x = \"Vehicle type\")\n\n\n\n\n\n\n\nFigure 4: Incidents by vehicle type\n\n\n\n\n\nWithout knowing more about how common different types of vehicles are in NYC, we can’t make strong inferences from this data; i.e., the prevalence of sedans, SUVs, and station wagons in the crash data likely reflects the prevalence of these vehicles on the road, period. If we wanted to explore whether certain vehicle types are disproportionately likely to be involved in crashes, we’d need to know not just how many of each vehicle type are present in the city, but also how many hours each vehicle type is in motion on average (since, for example, buses and box trucks probably spend more of their time driving than private passenger vehicles, even if there are more of the latter).\n\n\nWhat contributing factors were involved in crashes?\n\ncrashes_by_factor %&gt;% \n  filter(Contributing_factor != \"Unspecified\") %&gt;% # Ignore missing data\n  ggplot(aes(x = fct_rev(fct_inorder(Contributing_factor)), y = Crashes)) + \n  geom_col() + coord_flip() +\n  labs(x = \"Contributing factor\")\n\n\n\n\n\n\n\nFigure 5: Incidents by contributing factor\n\n\n\n\n\nHere, we can draw more inferences from the data alone. Distracted driving seems to be a clear issue, as well as following too closely and failing to yield.\n\n\nHow many vehicles were involved per crash?\nWe can also examine how many vehicles were involved per crash, and plot the distribution:\n\ncount(vehicle_dat, Collision_id, name = \"Vehicles\") %&gt;% # Count vehicles per collision\n  count(Vehicles, name = \"Crashes\") %&gt;% # Count collisions for each number of vehicles\n  mutate(Proportion = Crashes/sum(Crashes),\n         Label = paste0(scales::percent(Proportion),\"\\n(n=\",Crashes,\")\")) %&gt;% \n  ggplot(aes(x = Vehicles, y = Crashes, label = Label)) + \n  geom_col() + labs(x = \"Number of vehicles involved\") +\n  geom_text(nudge_y = 3000, size = 3)\n\n\n\n\n\n\n\nFigure 6: Incidents by number of vehicles involved\n\n\n\n\n\nPerhaps unsurprisingly, the majority of crashes involved two vehicles; crashes involving three or more vehicles were relatively rare (&lt;10% of crashes)."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#mapping",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#mapping",
    "title": "NYC Crash Data",
    "section": "Mapping",
    "text": "Mapping\nFinally, let’s map the data! We’ll begin by filtering out crashes missing location data, or with location (0,0):\n\ncrash_map_dat &lt;- filter(crash_dat, Latitude != 0, Longitude != 0)\n\nThe easiest way to check for outliers is simply to plot the data:\n\nggplot(crash_map_dat, aes(x=Longitude, y=Latitude)) + \n  geom_point(size=0.05, alpha = 0.5) # Small/transparent to handle overplotting\n\n\n\n\n\n\n\nFigure 7: Crash locations\n\n\n\n\n\nThe projection is a little wonky, but we can see the map taking shape. There are enough data points in our data set to make individual streets!\n\nIndividual crashes\nWe can improve our map by incorporating actual map data for New York City. Fortunately, most of the heavy lifting has already been done for us by the nycgeo package! Among other things, this package can split the geography up according to different types of administrative boundaries, from boroughs all the way down to invidual census tracts.\n\n# Load NYC map data (https://nycgeo.mattherman.info/index.html)\nmap_dat &lt;- nyc_boundaries(geography = \"tract\") # Split into census tracts\n\n# Add simple features (sf) to our data set\ncrash_map_dat &lt;- crash_map_dat %&gt;% \n  st_as_sf(coords = c(\"Longitude\",\"Latitude\"), \n           crs=4326,\n           stringsAsFactors = FALSE)\n\nWe can now overlay individual crashes as points on a map of census tracts:\n\nggplot(data = crash_map_dat) + \n  geom_sf(data = map_dat, mapping = NULL) +\n  geom_sf(size = 0.025, alpha = 0.25, color = \"red\") + \n  theme_void()\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n\nFigure 8: Crash locations\n\n\n\n\n\nPlotting crashes as individual points is useful for identifying where crashes occurred, but things get muddled by the sheer number of data points, especially where the data get dense (e.g., basically all of Manhattan).\n\n\nChoropleth map\nThe solution is to aggregate the data by dividing the map into subsections and coloring them based on the number of crashes. We’ll define the subsections based on census tracts, thus making a choropleth map!\nWe’ll also need a way to identify which tract each crash occurred in, since our data only includes the lat/lon of the crash. Fortunately, nyc_point_poly() will do just this!\n\ncrash_map_dat_tract &lt;- nyc_point_poly(crash_map_dat, \"tract\") %&gt;% \n  st_set_geometry(NULL)\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\nTrasnsforming points to EPSG 2263\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\ncrashes_by_tract &lt;- crash_map_dat_tract %&gt;%\n  count(tract_id, name = \"Crashes\", sort = TRUE)\n\n\nhead(crashes_by_tract, n=10)\n\n\n\nTable 3: Crashes per tract\n\n\n\n\n  \n\n\n\n\n\n\n\nNow we’ll join this with our map data and plot:\n\nleft_join(map_dat,crashes_by_tract) %&gt;% \n  ggplot() + \n  geom_sf(aes(fill = Crashes)) +\n  scale_fill_viridis_c(option = \"A\") +\n  coord_sf() +\n  theme_void() + \n  theme(legend.position = c(0.2,0.815)) # Position legend in blank area of plot\n\nJoining with `by = join_by(tract_id)`\nold-style crs object detected; please recreate object with a recent\nsf::st_crs()\n\n\n\n\n\n\n\n\nFigure 9: Crashes by census tract\n\n\n\n\n\nWe can use a similar approach to plot other variables by tract. Let’s look at the total number of injuries reported per tract:\n\ninjuries_by_tract &lt;- crash_map_dat_tract %&gt;% \n  group_by(tract_id) %&gt;% \n  summarise(Injuries =sum(Number_of_persons_injured)) %&gt;% \n  ungroup()\n\nhead(injuries_by_tract, n=10)\n\n\n  \n\n\n\n\nleft_join(map_dat,injuries_by_tract) %&gt;% \n  ggplot() + \n  geom_sf(aes(fill = Injuries)) +\n  scale_fill_viridis_c(option = \"A\") +\n  coord_sf() +\n  theme_void() + \n  theme(legend.position = c(0.2,0.815)) # Position legend in blank area of plot\n\nJoining with `by = join_by(tract_id)`\nold-style crs object detected; please recreate object with a recent\nsf::st_crs()\n\n\n\n\n\n\n\n\nFigure 10: Injuries by census tract\n\n\n\n\n\n\n\nHeatmap\nFor comparison, we can also make a heatmap. Same basic idea as the choropleth map (i.e., colors represent number of crashes per area), but the areas are formed by dividing the geography up into regular polygons, rather than using real-world divisions like census tracts.\n\nggplot() + \n  geom_sf(data = map_dat, mapping = NULL) +\n  geom_hex(data = filter(crash_dat, Latitude != 0, Longitude != 0), \n           aes(x=Longitude,y=Latitude),\n           binwidth=0.005) +\n  coord_sf(crs = 4326) +\n  scale_fill_viridis_c(option = \"A\") +\n  theme_void()+\n  theme(legend.position = c(0.2,0.815))\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n\nFigure 11: Crashes by location (heatmap)\n\n\n\n\n\nPersonally, I like the choropleth map better; it’s easier to see how the distribution of crashes maps onto the actual geography of the city.\n\n\nCrashes by street and tract\nIn both of our choropleth maps, we can see a couple of tracts with high rates of crashes relative to the surrounding areas (e.g., the long tract between Brooklyn and Queens that seems to correspond to Flushing Meadows Corona Park). I’m guessing that some of these high rates of crashes may be due to freeways and expressways running through the tracts in question. Conveniently, the data set includes the names of the street on which each crash occurred! Let’s look at the street with the most crashes in the top 20 tracts by crashes:\n\ncrash_map_dat_tract %&gt;%\n  count(tract_id, On_street_name, name = \"Crashes\", sort = TRUE) %&gt;% \n  drop_na(On_street_name) %&gt;% \n  filter(tract_id %in% head(crashes_by_tract$tract_id,20)) %&gt;% # Top 20 tracts\n  slice_max(order_by = Crashes, n=1, by = tract_id) # Street w/ most crashes\n\n\n\nTable 4: Street with most crashes per tract\n\n\n\n\n  \n\n\n\n\n\n\nAs expected, in the tracts with the most crashes, the street with the most crashes tends to be a parkway or expressway.\nWhile we’re at it, let’s look at the top 50 streets for crashes citywide. We’ll use the full data set (crash_dat) to include cases where the street was recorded but the latitude/longitude were not.\n\ncrash_dat %&gt;% \n  drop_na(On_street_name) %&gt;% \n  mutate(On_street_name = str_to_title(On_street_name)) %&gt;% \n  count(On_street_name, sort = TRUE, name = \"Crashes\") %&gt;% \n  head(n=50)\n\n\n\nTable 5: Street with most crashes citywide"
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#footnotes",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#footnotes",
    "title": "NYC Crash Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn general, vroom::vroom() is much faster than read.csv(), but here we’re also limited by connection speed; it took a little over a minute to read the data in with this approach.↩︎\nThere are a number of approaches for dealing with very large (and larger-than-memory) data sets, which I hope to cover in future posts.↩︎\nObligatory reference to that one Joy Division album cover.↩︎"
  },
  {
=======
>>>>>>> 5592f3f (Draft Intro to R workshop)
    "objectID": "posts/2025-01-23-IMBD-analyses/index.html",
    "href": "posts/2025-01-23-IMBD-analyses/index.html",
    "title": "IMDB Data Wrangling",
    "section": "",
    "text": "Today, I’ll explore some data on movies from IMDB and Box Office Mojo. I’ll begin with some quick data wrangling, then walk through modelling the data with the tidymodels package. I’ve been experimenting with this package a lot recently, and am particularly impressed by how it streamlines running multiple models and comparing the results. Note that this won’t be a full-fledged tutorial for tidymodels (the tidymodels docs cover that very well) nor a deep dive into the models themselves, but more of a worked example.\n# Load packages\nlibrary(tidymodels) # Modelling\nlibrary(broom) # Extracting coefficients\nlibrary(yardstick) # Evaluating models\nlibrary(vip) # Variable importance\n\nlibrary(rvest) # For webscraping\n\nlibrary(tidyverse) # Data wrangling\nlibrary(ggrepel) # Extra data viz\n\n# Set theme for plots\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "posts/2025-01-23-IMBD-analyses/index.html#imdb",
    "href": "posts/2025-01-23-IMBD-analyses/index.html#imdb",
    "title": "IMDB Data Wrangling",
    "section": "IMDB",
    "text": "IMDB\nThe IMDB Non-Commercial Datasets page lists the data sets available, as well as a dictionary for each one. Six data sets contain information about titles (movie, TV show, video game, etc.) and one contains info about individuals. These data sets are quite large; the ratings data includes about 1.5 million titles, and the individuals data includes over 14 million names. To keep things tractable, we’ll limit our exploration to movies with at least 1000 individual ratings (“votes”).\n\n# Read ratings data\nratings_dat &lt;- read_tsv(\"https://datasets.imdbws.com/title.ratings.tsv.gz\",\n                        show_col_types = FALSE) %&gt;% \n  filter(numVotes &gt;= 1000) # Only titles with at least 1000 votes\n\n# Read names data\nnames_dat &lt;- read_tsv(\"https://datasets.imdbws.com/name.basics.tsv.gz\", \n                      show_col_types = FALSE,\n                      na = \"\\\\N\")\n\n# Read crew data\ncrew_dat &lt;- read_tsv(\"https://datasets.imdbws.com/title.crew.tsv.gz\", \n                     show_col_types = FALSE,\n                     na = \"\\\\N\")\n\nFor the main title data set (title.basics.tsv.gz), we’ll do a little more data cleaning up front. Specifically, we’ll:\n\nLimit ourselves to movies (i.e., excluding TV shows, video games, and other types of media)\nExclude adult content (keeping thing SFW!)\nRead release year and run time as integers\nSplit the genres column (currently a series of comma-separated tags) into multiple binary variables (1 if the tag applies, 0 otherwise)\n\nUsing lazy reading via the lazy = TRUE argument to read_tsv() helps to speed up this process, although in my experience the real bottleneck is downloading the data in the first place.\n\ntitle_dat &lt;- read_tsv(\"https://datasets.imdbws.com/title.basics.tsv.gz\",\n                      na = \"\\\\N\",\n                      show_col_types = FALSE,\n                      lazy = TRUE) %&gt;% # Don't load everything into memory!\n  filter(titleType == \"movie\", # Only movies\n         isAdult == 0) %&gt;% # Not adult!\n  mutate(startYear = as.integer(startYear),\n         runtimeMinutes = as.integer(runtimeMinutes)) %&gt;% \n  select(-c(isAdult,endYear)) %&gt;% # Drop unneeded columns\n  separate_longer_delim(genres, delim = \",\") %&gt;% # Split tags into multiple rows\n  mutate(temp=1) %&gt;% # Create column of 1s\n  pivot_wider(names_from = genres, # Make wider; one column per genre\n              values_from = temp,\n              values_fill = 0,\n              names_prefix = \"Genre_\") # Set naming convention for new columns\n\nWarning: There was 1 warning in `filter()`.\nℹ In argument: `titleType == \"movie\"`.\nCaused by warning:\n! One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n# Check work\ntitle_dat %&gt;% head(10)"
  },
  {
    "objectID": "posts/2025-01-23-IMBD-analyses/index.html#box-office-mojo",
    "href": "posts/2025-01-23-IMBD-analyses/index.html#box-office-mojo",
    "title": "IMDB Data Wrangling",
    "section": "Box Office Mojo",
    "text": "Box Office Mojo\nWe’ll supplement the titles data with information on lifetime gross. Conveniently, Box Office Mojo lists the top 1000 domestic films by lifetime gross; less conveniently, the information is in a paged table. To save a bunch of copying and pasting, we’ll do some basic webscraping to get this information into a single data frame.\n\n# Create vector of URLs, one for each page of the table\nURLs &lt;- paste0(\"https://www.boxofficemojo.com/chart/top_lifetime_gross/?offset=\", \n               0:4*200)\n\n# Loop over URLs\ntop_gross &lt;- map(\n  URLs, \\(url) \n  read_html(url) %&gt;% # For each URL...\n    html_elements(\"table\") %&gt;% # Pick out the table\n    html_table() %&gt;% # Read the table as a list\n    pluck(1) %&gt;% # Get the first element (the actual data)\n    # Remove commas and make into integers\n    mutate(Rank=str_remove(Rank,\"\\\\,\") %&gt;% as.integer(), \n           `Lifetime Gross`=str_remove_all(`Lifetime Gross`,\"\\\\,|\\\\$\") %&gt;% \n             as.integer())) %&gt;% \n  list_rbind() # Combine into single data frame\n\n# Check work\ntop_gross %&gt;% head(10)"
  },
  {
    "objectID": "posts/2025-01-23-IMBD-analyses/index.html#join-data-sets",
    "href": "posts/2025-01-23-IMBD-analyses/index.html#join-data-sets",
    "title": "IMDB Data Wrangling",
    "section": "Join data sets",
    "text": "Join data sets\nLet’s join our title (title_dat), ratings (ratings_dat), and domestic gross (top_gross) data sets into a single data frame. title_dat and ratings_dat will automatically join using the unique title identifiers (tconst) present in both data sets. For the domestic gross data, we’ll join by title and release year; because these variables have different names in the title data and domestic gross data (originalTitle versus Title and startYear versus Year), we’ll need to specify which columns in each data set are equivalent, using join_by().\nNote the use of inner_join() to join the title and rating data; this ensures that we retain only titles that have ratings associated with them (and at least 1000 votes, thanks to our earlier filtering of the ratings data). In contrast, we join the domestic gross data using left_join(), meaning that we retain titles from the title data even if they don’t appear in the domestic gross data.\nLastly, we’ll join crew_dat to get unique identifiers for each movie’s director, then join names_dat to get the actual names associated with each identifier1.\n\ntitle_dat &lt;- title_dat %&gt;% # Start with title data set  \n  inner_join(ratings_dat) %&gt;% # Join ratings data set (use tconst as key)\n  left_join(top_gross,  # Join gross data set\n            by = join_by(originalTitle == Title, # Col names differ -- specify!\n                         startYear == Year)) %&gt;% \n  left_join(crew_dat) %&gt;% # Join crew data set\n  left_join(select(names_dat, # Names!\n                   directors=nconst, # Rename variables to match\n                   Director=primaryName)) %&gt;%  \n  select(tconst, primaryTitle:runtimeMinutes, averageRating, numVotes, # Reorganize columns\n         `Lifetime Gross`, Director, starts_with(\"Genre\"))\n\nJoining with `by = join_by(tconst)`\nJoining with `by = join_by(tconst)`\nJoining with `by = join_by(directors)`"
  },
  {
    "objectID": "posts/2025-01-23-IMBD-analyses/index.html#descriptives",
    "href": "posts/2025-01-23-IMBD-analyses/index.html#descriptives",
    "title": "IMDB Data Wrangling",
    "section": "Descriptives",
    "text": "Descriptives\nLet’s check our work thus far via glimpse():\n\ntitle_dat %&gt;% glimpse()\n\nRows: 44,716\nColumns: 37\n$ tconst             &lt;chr&gt; \"tt0002130\", \"tt0002423\", \"tt0002844\", \"tt0003014\",…\n$ primaryTitle       &lt;chr&gt; \"Dante's Inferno\", \"Passion\", \"Fantômas: In the Sha…\n$ originalTitle      &lt;chr&gt; \"L'inferno\", \"Madame DuBarry\", \"Fantômas I: À l'omb…\n$ startYear          &lt;int&gt; 1911, 1919, 1913, 1913, 1913, 1913, 1913, 1914, 191…\n$ runtimeMinutes     &lt;int&gt; 71, 113, 54, 96, 61, 90, 85, 78, 148, 52, 59, 70, 6…\n$ averageRating      &lt;dbl&gt; 7.0, 6.6, 6.9, 7.0, 6.9, 6.9, 6.4, 6.4, 7.1, 6.0, 6…\n$ numVotes           &lt;dbl&gt; 3704, 1049, 2598, 1493, 1765, 1405, 2522, 1493, 409…\n$ `Lifetime Gross`   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Director           &lt;chr&gt; NA, \"Ernst Lubitsch\", \"Louis Feuillade\", \"Victor Sj…\n$ Genre_Romance      &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ Genre_Documentary  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_News         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Sport        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_NA           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Action       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, …\n$ Genre_Adventure    &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ Genre_Biography    &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Drama        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, …\n$ Genre_Fantasy      &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Comedy       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ Genre_War          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Crime        &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ Genre_Family       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_History      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Genre_Sci-Fi`     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Thriller     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Western      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Mystery      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Horror       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ Genre_Music        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Animation    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Musical      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Genre_Film-Noir`  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Genre_Talk-Show`  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Genre_Reality-TV` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Genre_Adult        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Genre_Game-Show`  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nWe see the following variables:\n\ntconst: unique identifier for each movie\nprimaryTitle: main title by which the movie is known \nstartYear: year movie was released\nruntimeMinutes: movie’s runtime, in minutes (natch)\naverageRating: average of all ratings (1-10 scale)\nnumVotes: total number of ratings received\nLifetime Gross: lifetime domestic gross of movie; only available for the top 1000 highest-grossing movies in our data set\nDirector: name of movie’s director; NA if more than one director\nGenre_X: genre tags (1 if the tag applies, 0 otherwise)\n\nWe can get a quick-and-dirty overview of our continuous variables via summary(), and a breakdown of the distributions of (and correlations2 between) these variables via GGally::ggpairs():\n\ntitle_dat %&gt;% \n  select(\"startYear\",\"runtimeMinutes\",\n         \"averageRating\",\"numVotes\",\"Lifetime Gross\") %&gt;% \n  summary()\n\n   startYear    runtimeMinutes  averageRating     numVotes      \n Min.   :1911   Min.   : 39.0   Min.   :1.00   Min.   :   1000  \n 1st Qu.:1989   1st Qu.: 91.0   1st Qu.:5.60   1st Qu.:   1673  \n Median :2008   Median :100.0   Median :6.40   Median :   3334  \n Mean   :2000   Mean   :105.2   Mean   :6.24   Mean   :  25486  \n 3rd Qu.:2017   3rd Qu.:115.0   3rd Qu.:7.10   3rd Qu.:  10645  \n Max.   :2025   Max.   :776.0   Max.   :9.80   Max.   :2994771  \n                NA's   :84                                      \n Lifetime Gross     \n Min.   : 85297000  \n 1st Qu.:105874210  \n Median :137275620  \n Mean   :174204573  \n 3rd Qu.:196461860  \n Max.   :936662225  \n NA's   :43757      \n\n\n\ntitle_dat %&gt;% \n  select(\"startYear\",\"runtimeMinutes\",\n         \"averageRating\",\"numVotes\",\"Lifetime Gross\") %&gt;% \n  GGally::ggpairs(\n    lower = list(continuous = GGally::wrap(\"points\", # Handle overplotting\n                                           alpha = 0.1, \n                                           size = 1)),\n    upper = list(continuous = GGally::wrap(GGally::ggally_cor, \n                                           method = \"spearman\",\n                                           use = \"pairwise\",\n                                           title = \"Rho\"))\n  ) + labs(title = \"Distributions and correlations of continuous variables\",\n           caption = \"Data from datasets.imdbws.com\")\n\n\n\n\n\n\n\n\nMajor takeaways:\n\nstartYear: most movies in the data set are relatively recent (post-2008)\nruntimeMinutes: extreme positive skew. Digging around in the raw data, I found a handful of cases where a miniseries was treated as a single movie, or where the date was copied into the runtime column, but also at least one 25-hour-long movie!\nnumVotes: extreme positive skew!\nCorrelations between these variables are statistically significant, but small enough that we shouldn’t have glaring issues with multicollinearity (although the correlation between numVotes and Lifetime Gross is potentially troublesome)."
  },
  {
    "objectID": "posts/2025-01-23-IMBD-analyses/index.html#visualization",
    "href": "posts/2025-01-23-IMBD-analyses/index.html#visualization",
    "title": "IMDB Data Wrangling",
    "section": "Visualization",
    "text": "Visualization\nLet’s start by examining which movies got the most votes overall. Because we joined the crew data, we can also include directors’ names in the plot!\n\n# Most votes overall\ntitle_dat %&gt;% \n  slice_max(order_by = numVotes, n = 20) %&gt;% # Top 20 films\n  mutate(Label = paste0(primaryTitle,\"\\n(\",Director,\")\")) %&gt;% # Add directors\n  ggplot(aes(x = fct_inorder(Label) %&gt;% fct_rev, y = numVotes)) + \n  geom_col(fill=\"dodgerblue\") + \n  geom_text(aes(label = format(numVotes,big.mark = \",\")), \n            color = \"white\", nudge_y = -3*10^5) +\n  coord_flip() + \n  scale_y_continuous(labels = scales::label_number(big.mark = \",\")) +\n  labs(x = \"Movie\", y=\"Total votes\", \n       title = \"Top 20 movies by number of votes\",\n       caption = \"Data from datasets.imdbws.com\")\n\n\n\n\n\n\n\n\nUnsurprisingly, we see a mix of (relatively) recent blockbusters such as Interstellar and The Wolf of Wall Street as well as older classics like The Godfather and Silence of the Lambs.\nNext, let’s look at the relationship between ratings and lifetime gross. We’ll use ggrepel::geom_text_repel() to label the highest-grossing films (those making more than $500 million).\n\ntitle_dat %&gt;% \n  drop_na(`Lifetime Gross`) %&gt;% \n  ggplot(aes(x = averageRating, y = `Lifetime Gross`)) + \n  geom_point()+\n  geom_text_repel(data = filter(title_dat, `Lifetime Gross` &gt; 500000000), \n                  aes(label = primaryTitle),\n                  color = \"firebrick\") +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  labs(x = \"Average rating\",\n       y = \"Lifetime domestic gross\",\n       title = \"Lifetime domestic gross by average rating\",\n       caption = \"Includes only top 1000 movies by lifetime domestic gross.\n       Data from datasets.imdbws.com and boxofficemojo.com/chart/top_lifetime_gross\")\n\nWarning: ggrepel: 1 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\nFinally, let’s look at the frequency of each genre tag in the data set:\n\ntitle_dat %&gt;% \n  select(starts_with(\"Genre_\")) %&gt;% \n  pivot_longer(everything(), names_prefix = \"Genre_\") %&gt;% \n  filter(value==1) %&gt;% \n  count(name, sort = TRUE) %&gt;% # Get frequency of each tag\n  ggplot(aes(x=fct_rev(fct_inorder(name)),y=n)) + # Order by frequency\n  geom_col(fill=\"darkblue\") + \n  geom_text(aes(y = ifelse(n&gt;3000, n-1500, n+1000), color = n &gt; 3000,\n    label = format(n,big.mark = \",\"))) +\n  scale_y_continuous(labels = scales::label_number(big.mark = \",\")) +\n  coord_flip() +\n  scale_color_manual(values = c(\"black\",\"white\")) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Genre\", y = \"Frequency\",\n       title = \"Frequency of genre tags\",\n       caption = \"Data from datasets.imdbws.com\")\n\n\n\n\n\n\n\n\nDrama and comedy are the most frequent tags by a pretty substantial margin!"
  },
  {
    "objectID": "posts/2025-01-23-IMBD-analyses/index.html#lifetime-gross",
    "href": "posts/2025-01-23-IMBD-analyses/index.html#lifetime-gross",
    "title": "IMDB Data Wrangling",
    "section": "Lifetime gross",
    "text": "Lifetime gross\nTo start, let’s make a model to investigate which variables best predict lifetime gross. Because we only have lifetime gross data for the 1000 highest grossing movies in our data set, this will substantially decrease our sample size:\n\n# Define our data set\ngross_dat &lt;- title_dat %&gt;% \n  filter(runtimeMinutes &lt; 300, # Drop movies over 5hrs long...\n         !is.na(startYear), # Those missing release year...\n         !is.na(`Lifetime Gross`)) %&gt;% # And those missing gross data\n  select(tconst,\n         primaryTitle,\n         gross = `Lifetime Gross`,\n         averageRating,\n         numVotes,\n         startYear, \n         runtimeMinutes, \n         starts_with(\"Genre_\")) %&gt;% \n  select(-c('Genre_News', 'Genre_NA', # Drop columns for genres that don't occur\n            'Genre_Talk-Show', 'Genre_Reality-TV', 'Genre_Game-Show'))\n\nglimpse(gross_dat)\n\nRows: 959\nColumns: 30\n$ tconst            &lt;chr&gt; \"tt0029583\", \"tt0031381\", \"tt0034492\", \"tt0042332\", …\n$ primaryTitle      &lt;chr&gt; \"Snow White and the Seven Dwarfs\", \"Gone with the Wi…\n$ gross             &lt;int&gt; 184925486, 200882193, 102247150, 93141149, 87404651,…\n$ averageRating     &lt;dbl&gt; 7.6, 8.2, 7.3, 7.3, 7.3, 7.3, 7.3, 7.8, 7.9, 8.1, 8.…\n$ numVotes          &lt;dbl&gt; 221758, 342617, 158217, 178774, 157776, 153329, 1875…\n$ startYear         &lt;int&gt; 1937, 1939, 1942, 1950, 1953, 1955, 1961, 1964, 1965…\n$ runtimeMinutes    &lt;int&gt; 83, 238, 69, 74, 77, 76, 79, 139, 197, 172, 106, 78,…\n$ Genre_Romance     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0…\n$ Genre_Documentary &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Sport       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Action      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ Genre_Adventure   &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ Genre_Biography   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0…\n$ Genre_Drama       &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0…\n$ Genre_Fantasy     &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Comedy      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0…\n$ Genre_War         &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Crime       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0…\n$ Genre_Family      &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_History     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Genre_Sci-Fi`    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Thriller    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ Genre_Western     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Mystery     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Horror      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Genre_Music       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Animation   &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ Genre_Musical     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Genre_Film-Noir` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Adult       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nNext, we’ll define a recipe to preprocess our data. This recipe will:\n\nPredict lifetime gross (gross) from all other predictors\nTreat tconst and primaryTitle as identifiers\nLog-transform gross and numVotes to handle deviation from normality\nNormalize all continuous predictors (i.e., convert to z-scores)\nRemove predictors with no variance\n\n\n# Set the recipe\nlm_gross_rec &lt;- recipe(gross ~ ., gross_dat) %&gt;% # Predict gross from all others\n  update_role(tconst,primaryTitle,new_role = \"ID\") %&gt;% # Treat as identifiers\n  step_log(gross, numVotes) %&gt;% # Log transform gross and number of votes\n  step_normalize(startYear,runtimeMinutes,numVotes,gross) %&gt;% # Normalize\n  step_zv(all_predictors()) # Remove predictors with no variance\n\nFinally, we’ll fit the model:\n\n# Fit model\nlm_gross_fit &lt;- workflow() %&gt;% \n  add_model(linear_reg()) %&gt;% # Use linear regression\n  add_recipe(lm_gross_rec) %&gt;% # Pre-process data via our recipie\n  fit(data = gross_dat) # Fit using our data set\n\nlm_gross_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_log()\n• step_normalize()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n      (Intercept)      averageRating           numVotes          startYear  \n        -0.472949           0.039284           0.432938           0.112942  \n   runtimeMinutes      Genre_Romance  Genre_Documentary        Genre_Sport  \n         0.175688           0.009673           0.988917           0.050235  \n     Genre_Action    Genre_Adventure    Genre_Biography        Genre_Drama  \n         0.036221           0.341288          -0.202291          -0.134246  \n    Genre_Fantasy       Genre_Comedy          Genre_War        Genre_Crime  \n         0.231324           0.107008          -0.584406          -0.183351  \n     Genre_Family      Genre_History     `Genre_Sci-Fi`     Genre_Thriller  \n         0.067307          -0.197419           0.051611          -0.175678  \n    Genre_Western      Genre_Mystery       Genre_Horror        Genre_Music  \n        -0.132603          -0.362759           0.186564           0.113136  \n  Genre_Animation      Genre_Musical  \n         0.365934           0.154279  \n\n\nTo make sense of the model results, let’s visualize our coefficients via a dot-and-whisker plot:\n\nbroom.mixed::tidy(lm_gross_fit, conf.int = TRUE) %&gt;% # Get coefficients and CIs\n  drop_na(estimate) %&gt;% \n  arrange(desc(estimate)) %&gt;% \n  dotwhisker::dwplot(dot_args = list(size = 2, color = \"black\"),\n                     whisker_args = list(color = \"black\"),\n                     vline = geom_vline(xintercept = 0, \n                                        colour = \"grey50\", \n                                        linetype = 2)) + \n  labs(title = \"Predicting lifetime gross\", subtitle = \"Linear model\")\n\n\n\n\n\n\n\n\nLooking at the first few variables, a film being a documentary, animation, adventure, and/or fantasy seems to predict higher lifetime gross. Similarly, more votes overall, longer runtimes, and more recent release dates also seem to predict higher lifetime gross. The fact that being a documentary predicts higher lifetime gross seems a little odd – I don’t usually think of documentaries as huge moneymakers. To investigate, let’s see what documentaries are included in our data set for this model:\n\ngross_dat %&gt;% \n  filter(Genre_Documentary == 1) %&gt;% \n  select(primaryTitle:runtimeMinutes)\n\n Predicting lifetime gross - Documentaries\n  \n\n\n\nCrucially, we only have four documentaries in our entire data set – this explains the very wide confidence interval for this coefficient in our dot-and-whisker plot. More to the point, the documentaries we have are an IMAX 3D feature about the International Space Station, a controversial Michael Moore documentary, Jackass 3D (self-explanatory), and a chronicle of Taylor Swift’s Eras Tour. Overall, I’d say we’re looking at a specific type of selection bias; the documentaries that make it into the top 1000 grossing films aren’t necessarily representative of documentaries in general.\nLastly, let’s look at the ten highest and lowest residuals in our results; these are the movies whose lifetime gross was most underestimated and overestimated (respectively) by the model:\n\naugment(lm_gross_fit, gross_dat) %&gt;% \n  slice_max(.resid, n=10) %&gt;% \n  select(.resid, .pred, primaryTitle:runtimeMinutes)\n\n Predicting lifetime gross - Highest residuals\n  \n\n\n\n\naugment(lm_gross_fit, gross_dat) %&gt;% \n  slice_min(.resid, n=10) %&gt;% \n  select(.resid, .pred, primaryTitle:runtimeMinutes)\n\n Predicting lifetime gross - Lowest residuals"
  },
  {
    "objectID": "posts/2025-01-23-IMBD-analyses/index.html#average-ratings",
    "href": "posts/2025-01-23-IMBD-analyses/index.html#average-ratings",
    "title": "IMDB Data Wrangling",
    "section": "Average ratings",
    "text": "Average ratings\nNext up, let’s see if we can predict a movie’s average rating from its number of votes, release year, run time, and genre tags. To start, we’ll create a new data frame by filtering out movies over 5 hours long and those missing a release year, and selecting only the variables we’re interested in.\n\n# Create data set\nrating_model_dat &lt;- title_dat %&gt;% \n  filter(runtimeMinutes &lt; 300,\n         !is.na(startYear)) %&gt;% \n  select(tconst,\n         primaryTitle,\n         averageRating,\n         numVotes,\n         startYear, \n         runtimeMinutes, \n         starts_with(\"Genre_\")) %&gt;% \n  select(-c('Genre_News', 'Genre_NA', \n            'Genre_Talk-Show', 'Genre_Reality-TV', 'Genre_Game-Show'))\n\nglimpse(rating_model_dat)\n\nRows: 44,608\nColumns: 29\n$ tconst            &lt;chr&gt; \"tt0002130\", \"tt0002423\", \"tt0002844\", \"tt0003014\", …\n$ primaryTitle      &lt;chr&gt; \"Dante's Inferno\", \"Passion\", \"Fantômas: In the Shad…\n$ averageRating     &lt;dbl&gt; 7.0, 6.6, 6.9, 7.0, 6.9, 6.9, 6.4, 6.4, 7.1, 6.0, 6.…\n$ numVotes          &lt;dbl&gt; 3704, 1049, 2598, 1493, 1765, 1405, 2522, 1493, 4095…\n$ startYear         &lt;int&gt; 1911, 1919, 1913, 1913, 1913, 1913, 1913, 1914, 1914…\n$ runtimeMinutes    &lt;int&gt; 71, 113, 54, 96, 61, 90, 85, 78, 148, 52, 59, 70, 60…\n$ Genre_Romance     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ Genre_Documentary &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Sport       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Action      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0…\n$ Genre_Adventure   &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ Genre_Biography   &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Drama       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1…\n$ Genre_Fantasy     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Comedy      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n$ Genre_War         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Genre_Crime       &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ Genre_Family      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_History     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Genre_Sci-Fi`    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Thriller    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Western     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Mystery     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Horror      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n$ Genre_Music       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Animation   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Musical     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `Genre_Film-Noir` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Genre_Adult       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nNext, we’ll split this data set into a training and a test set. The training set will used to train our models (hence the name); the test set will be used to evaluate how well they generalize to new data.\n\nset.seed(1337) # Set seed for reproducibility\nrating_data_split &lt;- initial_split(rating_model_dat, prop = 3/4)\nrating_data_train &lt;- training(rating_data_split)\nrating_data_test &lt;- testing(rating_data_split)\n\nNow we’ll specify a data pre-processing recipe that will:\n\nPredict average rating (averageRating) from all other predictors\nTreat tconst and primaryTitle as identifiers\nLog-transform numVotes to handle deviation from normality\nNormalize all continuous predictors (i.e., convert to z-scores)\n\n\n# Set the recipe\nrating_rec &lt;- recipe(averageRating ~ ., rating_model_dat) %&gt;% # Set formula\n  update_role(tconst, primaryTitle, new_role = \"ID\") %&gt;% # Treat as identifiers\n  step_log(numVotes) %&gt;% # Log transform to address deviation from normality\n  step_normalize(startYear, runtimeMinutes, numVotes) %&gt;% # Normalize continuous\n  step_zv(all_predictors()) # Remove predictors with no variance\n\nWe can reuse our work here with each model we run – this is one of the big advantages of tidymodels!\n\nLinear model\nFirst, let’s fit a linear model to our training data.\n\n# Fit model\nlm_rating_fit &lt;- workflow() %&gt;% # Set up workflow\n  add_model(linear_reg()) %&gt;% # Use linear regression\n  add_recipe(rating_rec) %&gt;%  # Use our pre-processing recipe\n  fit(data = rating_data_train) # Use training data set\n\n# Check results\nlm_rating_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_log()\n• step_normalize()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n      (Intercept)           numVotes          startYear     runtimeMinutes  \n        6.3188238          0.3084472         -0.3053317          0.2743756  \n    Genre_Romance  Genre_Documentary        Genre_Sport       Genre_Action  \n       -0.1205021          1.2775734         -0.1144554         -0.4181589  \n  Genre_Adventure    Genre_Biography        Genre_Drama      Genre_Fantasy  \n       -0.2825614          0.0420595          0.3336779         -0.2016875  \n     Genre_Comedy          Genre_War        Genre_Crime       Genre_Family  \n       -0.1212918         -0.0385408         -0.0321719         -0.1483447  \n    Genre_History     `Genre_Sci-Fi`     Genre_Thriller      Genre_Western  \n        0.0004387         -0.4777811         -0.1628879         -0.2258474  \n    Genre_Mystery       Genre_Horror        Genre_Music    Genre_Animation  \n       -0.0542027         -0.7826594         -0.0658001          0.8431900  \n    Genre_Musical  `Genre_Film-Noir`  \n       -0.1518250         -0.0612570  \n\n\nAs we did for the lifetime gross model, let’s visualize via a dot-and-whisker plot:\n\nbroom.mixed::tidy(lm_rating_fit, conf.int = TRUE) %&gt;% \n  drop_na(estimate) %&gt;% \n  arrange(desc(estimate)) %&gt;% \n  dotwhisker::dwplot(dot_args = list(size = 2, color = \"black\"),\n                     whisker_args = list(color = \"black\"),\n                     vline = geom_vline(xintercept = 0, colour = \"grey50\", linetype = 2)) + \n  labs(title = \"Predicting average ratings\", subtitle = \"Linear model\")\n\n\n\n\n\n\n\n\nOut of curiosity, let’s see the 20 movies with the lowest (most) negative residuals – in other words, the movies for which our model most overestimated ratings:\n\naugment(lm_rating_fit, rating_model_dat) %&gt;% \n  slice_min(.resid, n=20) %&gt;% \n  select(.pred, .resid, primaryTitle:runtimeMinutes)\n\n Predicting average ratings - Lowest residuals\n  \n\n\n\nThe presence of two Justin Bieber films near the top of the list jumps out at me, as do the titles “The Trump Prophecy” and “Buck Breaking”. If I had to guess, we’re seeing films that have attracted more (negative) attention due to their content than would be expected based purely on the variables available to our model. This highlights a crucial shortcoming of our model: other than the genre tags, it doesn’t incorporate any information about what the films are actually about.\nSame thing, but now the highest residuals:\n\naugment(lm_rating_fit, rating_model_dat) %&gt;% \n  slice_max(.resid, n=20) %&gt;% \n  select(.pred, .resid, primaryTitle:runtimeMinutes)\n\n Predicting average ratings - Highest residuals\n  \n\n\n\nThese appear to be mainly non-English titles, released recently, with relatively few votes (recall that our threshold for inclusion in the data set was 1000 votes).\n\n\nRandom forest\nWith only a few extra lines of code, we can run the same analysis using a different model! Let’s try a random forest model – all we need to do is set up the engine we want to use (ranger), specifying the mode (regression, since our outcome is continuous), the number of trees (trees), and the method to be used to assess variable importance (importance)3.\n\n# Set the engine\nrf_mod &lt;- rand_forest(mode = \"regression\", trees = 1000) %&gt;%\n  set_engine(\"ranger\", importance=\"impurity\") \n\n# Set workflow and fit model\nrf_rating_fit &lt;- workflow() %&gt;% \n  add_model(rf_mod) %&gt;% \n  add_recipe(rating_rec) %&gt;% # Same recipe as before! \n  fit(data = rating_data_train)\n\nrf_rating_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_log()\n• step_normalize()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      33456 \nNumber of independent variables:  25 \nMtry:                             5 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       0.7761085 \nR squared (OOB):                  0.442483 \n\n\n\n\nXGBoost\nWe’ll fit a third model using XGBoost:\n\n# Set the engine\nxgb_mod &lt;- boost_tree(engine = \"xgboost\", mode = \"regression\", trees = 1000)\n\n# Set workflow and fit model\nxgb_rating_fit &lt;- workflow() %&gt;% \n  add_model(xgb_mod) %&gt;% # Use XGBoost\n  add_recipe(rating_rec) %&gt;%  # Same recipe as before!\n  fit(data = rating_data_train)\n\nxgb_rating_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_log()\n• step_normalize()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 3.7 Mb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n    subsample = 1), data = x$data, nrounds = 1000, watchlist = x$watchlist, \n    verbose = 0, nthread = 1, objective = \"reg:squarederror\")\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", nthread = \"1\", objective = \"reg:squarederror\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 25 \nniter: 1000\nnfeatures : 25 \nevaluation_log:\n  iter training_rmse\n &lt;num&gt;         &lt;num&gt;\n     1     4.1600833\n     2     2.9903320\n   ---           ---\n   999     0.4058402\n  1000     0.4054915\n\n\n\n\nCompare\n\nMetrics\nTo compare our models, we first define a set of metrics:\n\nRoot mean square deviation (rmse): measures the deviation between observed and model-predicted values. Smaller values represent better fit; a value of 0 (impossible in practice) would represent perfect fit.\nR-squared (rsq): proportion of variance in outcome variable predicted by the model. Ranges from 0.0 to 1.0; larger values represent better fit, with a value of 1.0 (impossible in practice) representing perfect fit.\n\n\n# Define metrics\nrating_metrics &lt;- metric_set(rmse, rsq)\n\nNext, we loop over a list of our models, using the augment() function to calculate these metrics for each model. First, let’s see what happens if we calculate these metrics using the training data:\n\n# Make list of models\nrating_models &lt;- list(\"Linear\"=lm_rating_fit,\n                      \"Random Forest\"=rf_rating_fit,\n                      \"XGBoost\" = xgb_rating_fit)\n\nrating_models %&gt;% \n  map(\\(model) \n      augment(model, rating_data_train) %&gt;% # Use training data\n        rating_metrics(averageRating, .pred)) %&gt;% \n  list_rbind(names_to = \"Model\") %&gt;% \n  select(-.estimator) %&gt;% \n  pivot_wider(names_from = .metric, values_from = .estimate)\n\n Predicting average ratings - Model metrics (training data)\n  \n\n\n\nXGBoost appears superior by both metrics, but there’s a big caveat – models such as XGBoost (and random forests, to a lesser extent) are prone to overfitting, meaning that their predictions can be hyper-specialized for the data they were trained upon (e.g., rating_data_train) at the expense of their ability to generalize to novel data. To check, we’ll calculate our metrics again, but this time we’ll use our test data (rating_data_test):\n\nrating_models %&gt;% \n  map(\\(model) \n      augment(model, rating_data_test) %&gt;% # Use test data\n        rating_metrics(averageRating, .pred)) %&gt;% \n  list_rbind(names_to = \"Model\") %&gt;% \n  select(-.estimator) %&gt;% \n  pivot_wider(names_from = .metric, values_from = .estimate)\n\n Predicting average ratings - Model metrics (test data)\n  \n\n\n\nNote that our linear model performs roughly the same, but our random forest and XGBoost models show decreased performance. In particular, while XGBoost appeared to outperform the random forest model on the training data, it performs roughly equivalent to the linear model on the test data.\nThe takeaway here is that estimating our metrics based on the training data inflated the performance of our random forest and (in particular) XGBoost models; using the test data gives a much more accurate estimate of our metrics.\n\n\nVariable importance\nWe can use the vip package to get estimates of variable importance from each of our models, then plot them together:\n\nrating_models %&gt;% \n  map(\\(model) vip::vi(model,scale = T)) %&gt;% # Scale makes estimates comparable\n  list_rbind(names_to = \"Model\") %&gt;% \n  ggplot(aes(x=fct_reorder(Variable, Importance, mean), # Order by mean\n             y = Importance, \n             fill = Model, color = Model, shape = Model)) + \n  geom_point(alpha=0.8, size=3) +\n  coord_flip() + \n  labs(title = \"Predicting average rating - Variable importance\",\n       x = \"Variable\", y = \"Importance (scaled)\",\n       caption = \"Variables in descending order of mean importance\")\n\n\n\n\n\n\n\n\nThere’s some divergence between the three models, but overall, number of votes, run time, and release year seem to be the most important predictors across models.\n\n\nPredicted versus observed ratings\nLastly, let’s compare the predicted versus observed ratings for each of our three models:\n\nrating_models %&gt;% \n  map(\\(model) augment(model, rating_data_test)) %&gt;% \n  list_rbind(names_to=\"Model\") %&gt;% \n  ggplot(aes(x=.pred,y=averageRating))+\n  geom_hex()+\n  geom_abline(slope = 1) + \n  coord_fixed() + \n  scale_fill_viridis_c()+\n  labs(x=\"Predicted average rating\",y=\"Average rating\", fill = \"Frequency\",\n       title = \"Predicting average rating - Predicted versus observed ratings\")+\n  facet_wrap(~Model,nrow = 2) + \n  theme(legend.position = \"inside\", legend.position.inside = c(.75,.25))\n\n\n\n\n\n\n\n\nThe more closely the distribution follows the diagonal line, the more closely the predicted ratings match the observed ratings. Overall, the XGBoost model seems to have a slight advantage."
  },
  {
    "objectID": "posts/2025-01-23-IMBD-analyses/index.html#predicting-genre",
    "href": "posts/2025-01-23-IMBD-analyses/index.html#predicting-genre",
    "title": "IMDB Data Wrangling",
    "section": "Predicting genre",
    "text": "Predicting genre\nAll of our models thus far have examined continuous outcomes. Now, let’s try some models with a categorical outcome. Specifically, let’s see if we can predict whether or not a movie is classified as a drama based on the variables available to us! As with the previous example, we’ll run three models and compare the results: binary logistic regression (GLM), random forest, and XGBoost.\n\n# Define our data set\ndrama_model_dat &lt;- rating_model_dat %&gt;% \n  mutate(Drama = factor(Genre_Drama, labels = c(\"no\",\"yes\"))) %&gt;% \n  select(-Genre_Drama)\n\n# Split data into training/testing sets\n# Setting `strata = Drama` ensures that proportion of positive cases\n# is roughly equal in training vs test set\nset.seed(1337)\ngenre_data_split &lt;- initial_split(drama_model_dat, strata = Drama)\ngenre_data_train &lt;- training(genre_data_split)\ngenre_data_test &lt;- testing(genre_data_split)\n\n# Set the recipe\ngenre_rec &lt;- recipe(Drama ~ ., genre_data_train) %&gt;% \n  step_log(numVotes) %&gt;% \n  step_normalize(averageRating,startYear,runtimeMinutes,numVotes) %&gt;% \n  update_role(tconst,primaryTitle,new_role = \"ID\") # Treat as identifiers\n\n\nGLM\nSetting up the engine for GLM is straightforward; we technically don’t need to specify mode or engine, since logistic_reg() defaults to mode = \"classification\" and engine = \"glm\", but I’ve included them here for completeness.\n\n# Set the engine\ngenre_mod &lt;- logistic_reg(mode = \"classification\",\n                          engine = \"glm\")\n\n# Set the workflow\ngenre_workflow &lt;- workflow() %&gt;% \n  add_model(genre_mod) %&gt;% \n  add_recipe(genre_rec)\n\n# Fit model\ngenre_fit &lt;- genre_workflow %&gt;% \n  fit(data = genre_data_train)\n\n# Check results\ngenre_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_log()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n      (Intercept)      averageRating           numVotes          startYear  \n          3.75923            0.45874            0.09723            0.10283  \n   runtimeMinutes      Genre_Romance  Genre_Documentary        Genre_Sport  \n          0.20793           -0.54392           -6.32332           -0.89793  \n     Genre_Action    Genre_Adventure    Genre_Biography      Genre_Fantasy  \n         -1.91524           -1.75050           -0.18449           -1.18757  \n     Genre_Comedy          Genre_War        Genre_Crime       Genre_Family  \n         -3.16581           -0.39853           -0.71781           -1.32834  \n    Genre_History     `Genre_Sci-Fi`     Genre_Thriller      Genre_Western  \n         -0.37178           -2.00270           -2.29839           -2.35082  \n    Genre_Mystery       Genre_Horror        Genre_Music    Genre_Animation  \n         -0.94367           -2.74321           -0.47075           -2.35175  \n    Genre_Musical  `Genre_Film-Noir`        Genre_Adult  \n         -1.57199           -0.31297                 NA  \n\nDegrees of Freedom: 33454 Total (i.e. Null);  33429 Residual\nNull Deviance:      46010 \nResidual Deviance: 28000    AIC: 28050\n\n\n\n\nRandom forest\nFor a random forest model, we’ll pass a few more arguments to set up the engine.\n\n# Set the engine\ngenre_rf &lt;- rand_forest(trees = 1000) %&gt;%\n  set_engine(\"ranger\", importance=\"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n# Set the workflow\ngenre_workflow_rf &lt;- workflow() %&gt;% \n  add_model(genre_rf) %&gt;% \n  add_recipe(genre_rec)\n\n# # Finalize model and fit\ngenre_fit_rf &lt;- genre_workflow_rf %&gt;%\n  fit(data = genre_data_train)\n\n# Check results\ngenre_fit_rf\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_log()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  1000 \nSample size:                      33455 \nNumber of independent variables:  26 \nMtry:                             5 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1170917 \n\n\n\n\nXGBoost\nLastly, let’s run an XGBoost model:\n\ngenre_xgb &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"classification\")\n\n# Set the workflow\ngenre_workflow_xgb &lt;- workflow() %&gt;% \n  add_model(genre_xgb) %&gt;% \n  add_recipe(genre_rec)\n\n# Finalize model and fit\ngenre_fit_xgb &lt;- genre_workflow_xgb %&gt;% \n  fit(data = genre_data_train)\n\ngenre_fit_xgb\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_log()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 61.5 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n    subsample = 1), data = x$data, nrounds = 15, watchlist = x$watchlist, \n    verbose = 0, nthread = 1, objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 26 \nniter: 15\nnfeatures : 26 \nevaluation_log:\n  iter training_logloss\n &lt;num&gt;            &lt;num&gt;\n     1        0.5921049\n     2        0.5357200\n   ---              ---\n    14        0.3776050\n    15        0.3730612\n\n\n\n\nCompare\n\nMetrics\nAs previously, we’ll define a set of metrics to compare our model:\n\nAccuracy (accuracy): measures the proportion of cases that are predicted correctly. Values closer to 1.00 indicate greater accuracy.\nKappa (kap): like accuracy, but adjusted based on the proportion of correct predictions that would be expected due to chance alone.\nSensitivity (sensitivity): the number of predicted positives divided by the number of actual positives. In our case, what proportion of films with the “drama” tag did our models correctly predict as dramas?\nSpecificity (specificity): the number of predicted negatives divided by the number of actual negatives. In our case, what proportion of non-drama films did our models correctly predict as non-dramas?\n\n\n# Define metrics\ngenre_metrics &lt;- metric_set(\n  yardstick::accuracy,\n  kap, \n  sensitivity,\n  specificity\n)\n\n# Make list of models, augment with predictions\ngenre_models_augmented &lt;- list(\"GLM\"=genre_fit,\n                               \"Random Forest\"=genre_fit_rf,\n                               \"XGBoost\"=genre_fit_xgb) %&gt;% \n  map(\\(model) augment(model, genre_data_test)) # Get predictions from test dat\n  \n# Get metrics\ngenre_models_augmented %&gt;% \n  map(\\(model) genre_metrics(model, truth = Drama, estimate =.pred_class)) %&gt;% \n  list_rbind(names_to = \"Model\") %&gt;% \n  select(-.estimator) %&gt;% \n  pivot_wider(names_from = .metric, values_from = .estimate)\n\n Predicting drama - Model metrics\n  \n\n\n\nOur random forest and XGBoost models appear to perform better than our GLM, but don’t differ substantially from each other.\nWe can also assess our models in terms of their receiver operating characteristic (ROC) curves. We’ll run roc_curve() on each of our models, then bind the data together and plot with ggplot():\n\ngenre_models_augmented %&gt;% \n  map(~roc_curve(.x, truth = Drama, .pred_no)) %&gt;% # Calculate ROC curves\n  list_rbind(names_to = \"Model\") %&gt;% # Bind into single data frame\n  ggplot(aes(x = 1-specificity,y=sensitivity,color=Model)) + \n  geom_line() + # Plot the curves\n  geom_abline(linetype=3) + # Add diagonal line for reference\n  coord_fixed() + # Make plot square\n  labs(title=\"Predicting drama - ROC curves by model\")\n\n\n\n\n\n\n\n\nThese curves describe the relationship between a model’s true positive rate (sensitivity) and its false positive rate (1-specificity). If a model predicted a binary outcome purely at random, its true positive rate would always be equal to its false positive rate, and its “curve” would fall on the dotted diagonal line; the better a model predicts the outcome, the farther the curve curves away from the diagonal. Out of our three models, the random forest appears to slightly outperform the XGBoost, and both outperform the GLM, as seen from the fact that their curves bend farther from the diagonal (put another way, for any level of specificity, these models have equal or greater sensitivity than the GLM).\nWe can quantify the difference between the models’ ROC curves by calculating the area under the curve (AOC) for each model. Values closer to 1.00 indicate greater performance.\n\ngenre_models_augmented %&gt;% \n  map(\\(model) roc_auc(model, \n                       truth = Drama, \n                       .pred_yes, event_level=\"second\")) %&gt;% \n  list_rbind(names_to = \"Model\")\n\n Predicting drama - ROC AUC\n  \n\n\n\nHere again, the random forest model slightly outperforms the XGBoost model, and both outperform the GLM.\n\n\nVariable importance\nFinally, let’s plot estimates of variable importance for each model:\n\nlist(\"GLM\"=genre_fit,\n     \"Random Forest\"=genre_fit_rf,\n     \"XGBoost\"=genre_fit_xgb) %&gt;% \n  map(\\(model) vip::vi(model,scale = T)) %&gt;% \n  list_rbind(names_to = \"Model\") %&gt;% \n  ggplot(aes(x=fct_reorder(Variable, Importance, mean), y = Importance, \n             fill = Model, color = Model, shape = Model)) + \n  geom_point(size=3) +\n  coord_flip() +\n  labs(x=\"Variable\",\n       y=\"Importance (scaled)\",\n       title = \"Predicting drama - Variable importance\",\n       caption = \"Variables in descending order of mean importance\")\n\n\n\n\n\n\n\n\nThe genre tags for comedy and drama appear to be most important regardless of model; however, estimates of importance for variables like average rating, number of votes, and release year differ substantially by model (higher for random forest and XGBoost, lower for GLM). In a real-world scenario, we’d want to dig deeper, potentially using a permutation-based method to assess the variability of these estimates."
  },
  {
    "objectID": "posts/2025-01-23-IMBD-analyses/index.html#footnotes",
    "href": "posts/2025-01-23-IMBD-analyses/index.html#footnotes",
    "title": "IMDB Data Wrangling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis approach only works for films with a single director; we’ll stick with it here because the alternative would make our data structure more complicated.↩︎\nWe’ll use Spearman’s rho to estimate correlation since some of our variables aren’t normally distributed.↩︎\nMany models, including random forest and XGBoost, allow us to set hyperparameters such as the number of trees or the number of predictors to sample at each split. For this post, I’ve manually set the number of trees but left all other hyperparameters at their default values. I’ll cover methods for finding the optimal values for hyperparameters (knowing as tuning) in a future post; I’m skipping over it here because it can be both time- and processor-intensive!↩︎"
<<<<<<< HEAD
=======
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html",
    "href": "posts/2024-05-04-NYC-Crashes/index.html",
    "title": "NYC Crash Data",
    "section": "",
    "text": "In this post, I’ll explore a data set of motor vehicle crashes in New York City obtained via data.gov. This data set interested me for two reasons. First, it’s quite comprehensive, covering all crashes resulting in injury and/or over $1000 in damage since mid-2012. Second, it’s messy in a way that’s very representative of real-world data (and very hard to replicate with more “shrink-wrapped” data sets).\nMy goals for this analysis were to perform some basic data cleaning, get some insights from the data, and practice a type of visualization I don’t get to use very often in my day job: the choropleth map!\nTo start, we’ll load the packages we need:\n\n# Load packages\n## Handling dates\nlibrary(lubridate)\n\n## Mapping\nlibrary(sf)\nlibrary(nycgeo)\n\n## And starring...\nlibrary(tidyverse)\n\n# Set theme for plots\ntheme_set(theme_minimal())\n\nNow we’ll read in the data. Because the CSV is so large (~437 Mb), I decided to read it directly via the URL, rather than downloading and reading the file1. Once the data is loaded, we’ll tweak the column names for readability (converting to sentence case and replacing spaces with underscores), read the Crash_date column via lubridate::as_date(), and change the names of the columns containing vehicle type codes to be consistent with other column names. We’ll also filter the data to look only at crashes in 2023, to avoid using too much memory2.\n\n# Load crash data (https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv)\ncrash_url &lt;- \"https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv\"\n\n# Load data (takes a minute!)\ncrash_dat &lt;- vroom::vroom(file = crash_url, # Read from URL\n                          guess_max = 10^3,\n                          .name_repair = ~str_replace_all(str_to_sentence(.x), \n                                                          pattern = \" \", \n                                                          replacement = \"_\")) %&gt;%  \n  mutate(Crash_date= as_date(Crash_date, format = \"%m/%d/%Y\")) %&gt;% \n  filter(year(Crash_date) == 2023) %&gt;% # Only 2023 data\n  rename_with(\\(var) str_replace_all(var, \n                                     pattern = \"Vehicle_type_code\", \n                                     replacement = \"Type_code_vehicle\"))\n\nRows: 2084770 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (16): Crash_date, Borough, Location, On_street_name, Cross_street_name,...\ndbl  (12): Zip_code, Latitude, Longitude, Number_of_persons_injured, Number_...\ntime  (1): Crash_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#time-and-date",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#time-and-date",
    "title": "NYC Crash Data",
    "section": "Time and date",
    "text": "Time and date\nFirst, let’s look at incidents by month:\n\nggplot(crash_dat, aes(x = month(Crash_date, label = TRUE))) + \n  geom_bar() +\n  labs(x = \"Month\", y = \"Incidents\")\n\n\n\n\n\n\n\nFigure 1: Incidents by date\n\n\n\n\n\nNo obvious differences by month/season! Next, let’s consider crash times. Looking at raw crash time data, there seems to be an unusually high number of crashes at exactly midnight (00:00):\n\ncount(crash_dat, Crash_time,sort = TRUE) %&gt;% head(n=10)\n\n\n  \n\n\n\nIn fact, we see almost twice as many crashes reported at midnight as at 5pm! This makes me suspect that in at least some cases a crash time of 00:00 represents missing data; for the purposes of this graph, we’ll exclude them.\nNow to see whether the time that incidents occurred varied by month. To help us compare months, we’ll use a ridgeline plot3, via ggridges::geom_density_ridges():\n\ncrash_dat %&gt;% \n  filter(Crash_time &gt; 0) %&gt;% \n  mutate(Crash_month = month(Crash_date, label = TRUE)) %&gt;% \n  ggplot(aes(x=Crash_time, \n             color=Crash_month,\n             fill=Crash_month,\n             y = fct_rev(Crash_month))) + \n  ggridges::geom_density_ridges(alpha = 0.4,\n                                quantile_lines = TRUE,\n                                quantiles = 2) +\n  scale_color_viridis_d() +\n  scale_x_time(breaks = (0:6)*14400) +\n  labs(x = \"Time of day\", y = \"Month\", \n       caption = \"Vertical lines represent median.\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 2: Incidents by month and time of day\n\n\n\n\n\nCrashes by time of day look fairly consistent across months; notice that the median of each distribution is around 2pm. Additionally, we can see peaks in the number of incidents between 8-9am and between 5-6pm (representing rush hour, I’m assuming)."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#injuries-and-fatalities",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#injuries-and-fatalities",
    "title": "NYC Crash Data",
    "section": "Injuries and fatalities",
    "text": "Injuries and fatalities\nTo explore crash outcomes, we’ll use pivot_longer() to make a data frame with multiple rows per crash, each row representing the number of motorists, cyclists, or pedestrians (Type) who were injured or killed (Outcome) in that crash.\n\noutcomes_by_crash &lt;- crash_dat %&gt;% \n  select(Collision_id, \n         starts_with(\"Crash\"), # Keep date and time\n         starts_with(\"Number_of_\")) %&gt;%\n  pivot_longer(cols = starts_with(\"Number_of_\"), \n               names_prefix = \"Number_of_\", # Clean up col names\n               names_sep = \"\\\\_\", # Split at underscore\n               names_to = c(\"Type\",\"Outcome\")) %&gt;% \n  mutate(Crash_month = month(Crash_date, label = TRUE),\n         Type = str_to_title(Type),\n         Outcome = str_to_title(Outcome))\n\n# Check our work\nhead(outcomes_by_crash, n = 10)\n\n\n  \n\n\n\nNow we can plot total injuries/fatalities for motorists, cyclists, and pedestrians by month. Because fatalities are (thankfully) far rarer than injuries, we’ll use the scales = \"free_y\" argument to facet_wrap() to let the two halves of the plot use different y-axis limits. We’ll also use fct_reorder() to make sure the ordering of categories in the legend matches the ordering of the categories in the graph itself.\n\noutcomes_by_crash %&gt;% \n  filter(Type != \"Persons\") %&gt;% \n  mutate(Type = fct_reorder(Type, value, sum, .desc = T)) %&gt;% \n  ggplot(aes(x = Crash_month, \n             y = value, \n             color = Type,\n             group = Type)) + \n  stat_summary(geom=\"line\", fun = \"sum\") +\n  stat_summary(geom=\"point\", fun = \"sum\") + \n  facet_wrap(~Outcome,\n             scales = \"free_y\",\n             nrow = 2) + \n  labs(y = \"Number of persons\", x= \"Month\")\n\n\n\n\n\n\n\nFigure 3: Injuries and fatalities by month\n\n\n\n\n\nInterestingly, injuries and fatalities for motorists seem to be highest in summer, while injuries and fatalities for pedestrians are lower during these months."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#vehicles",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#vehicles",
    "title": "NYC Crash Data",
    "section": "Vehicles",
    "text": "Vehicles\nNext, let’s explore the contributing factors and types of vehicles involved.\n\nCleaning\nWe’ll restructure the data such that each row represents one vehicle, rather than one incident!\n\nvehicle_dat &lt;- crash_dat %&gt;% \n  select(Collision_id, starts_with(c(\"Contributing_factor\",\"Type_code\"))) %&gt;% \n  pivot_longer(-Collision_id, \n               names_sep = \"_vehicle_\", \n               names_to = c(\".value\",\"Vehicle\")) %&gt;% \n  drop_na() %&gt;% \n  mutate(Vehicle = as.factor(as.numeric(Vehicle)))\n\nNow we can look at the different vehicle type codes and contributing factors that occur in the data:\n\ncount(vehicle_dat, Type_code, sort = TRUE)\ncount(vehicle_dat, Contributing_factor, sort = TRUE)\n\n\n\nTable 1: Incidents by vehicle type and contributing factor (raw)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\nThere are a lot of categories in here, and many of them appear to overlap (e.g., “Motorcycle” and “Motorbike”). To clean up a bit, we’ll change all Type_code entries to sentence case, then manually consolidate some levels:\n\nvehicle_dat &lt;- vehicle_dat %&gt;% \n  mutate(Type_code = str_to_sentence(Type_code),\n         Type_code = case_when( # checks a series of conditionals\n           str_detect(Type_code, \n                      coll(\"wagon\", ignore_case = TRUE)) ~ \"SUV/station wagon\",\n           str_detect(Type_code, \n                      coll(\"sedan\",ignore_case = TRUE)) ~ \"Sedan\",\n           .default = Type_code),\n         Type_code = case_match( # Replaces (vectors of) matches, OLD ~ NEW\n           Type_code,\n           \"Bicycle\" ~ \"Bike\",\n           \"Motorbike\" ~ \"Motorcycle\",\n           c(\"Ambul\",\"Ambu\",\"Amb\") ~ \"Ambulance\",\n           c(\"Unkno\",\"Unk\") ~ \"Unknown\",\n           c(\"Fire\",\"Fdny\",\"Firetruck\",\n             \"Firet\",\"Fdny truck\",\"Fdny fire\") ~ \"Fire truck\",\n           \"E-sco\" ~ \"E-scooter\",\n           \"E-bik\" ~ \"E-bike\",\n           .default = Type_code) %&gt;% \n           fct_lump_prop(0.005), # Lump codes occurring less than 0.5% \n         Contributing_factor = fct_lump_prop(\n           str_to_sentence(Contributing_factor), \n           0.005)) # same for contributing factor\n\nNow let’s check our work:\n\n(crashes_by_type &lt;- count(vehicle_dat, Type_code, \n                          sort = TRUE, name = \"Crashes\"))\n(crashes_by_factor &lt;- count(vehicle_dat, Contributing_factor, \n                            sort = TRUE, name = \"Crashes\"))\n\n\n\nTable 2: Incidents by vehicle type and contributing factor\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\nIt’s not perfect, and a subject matter expert could probably identify more types/factors that could be combined, but it should be workable for our purposes.\n\n\nExploring\n\nWhat types of vehicles were involved in crashes?\n\nggplot(crashes_by_type, aes(x = fct_rev(fct_inorder(Type_code)), y = Crashes)) + \n  geom_col() + coord_flip() +\n  labs(x = \"Vehicle type\")\n\n\n\n\n\n\n\nFigure 4: Incidents by vehicle type\n\n\n\n\n\nWithout knowing more about how common different types of vehicles are in NYC, we can’t make strong inferences from this data; i.e., the prevalence of sedans, SUVs, and station wagons in the crash data likely reflects the prevalence of these vehicles on the road, period. If we wanted to explore whether certain vehicle types are disproportionately likely to be involved in crashes, we’d need to know not just how many of each vehicle type are present in the city, but also how many hours each vehicle type is in motion on average (since, for example, buses and box trucks probably spend more of their time driving than private passenger vehicles, even if there are more of the latter).\n\n\nWhat contributing factors were involved in crashes?\n\ncrashes_by_factor %&gt;% \n  filter(Contributing_factor != \"Unspecified\") %&gt;% # Ignore missing data\n  ggplot(aes(x = fct_rev(fct_inorder(Contributing_factor)), y = Crashes)) + \n  geom_col() + coord_flip() +\n  labs(x = \"Contributing factor\")\n\n\n\n\n\n\n\nFigure 5: Incidents by contributing factor\n\n\n\n\n\nHere, we can draw more inferences from the data alone. Distracted driving seems to be a clear issue, as well as following too closely and failing to yield.\n\n\nHow many vehicles were involved per crash?\nWe can also examine how many vehicles were involved per crash, and plot the distribution:\n\ncount(vehicle_dat, Collision_id, name = \"Vehicles\") %&gt;% # Count vehicles per collision\n  count(Vehicles, name = \"Crashes\") %&gt;% # Count collisions for each number of vehicles\n  mutate(Proportion = Crashes/sum(Crashes),\n         Label = paste0(scales::percent(Proportion),\"\\n(n=\",Crashes,\")\")) %&gt;% \n  ggplot(aes(x = Vehicles, y = Crashes, label = Label)) + \n  geom_col() + labs(x = \"Number of vehicles involved\") +\n  geom_text(nudge_y = 3000, size = 3)\n\n\n\n\n\n\n\nFigure 6: Incidents by number of vehicles involved\n\n\n\n\n\nPerhaps unsurprisingly, the majority of crashes involved two vehicles; crashes involving three or more vehicles were relatively rare (&lt;10% of crashes)."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#mapping",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#mapping",
    "title": "NYC Crash Data",
    "section": "Mapping",
    "text": "Mapping\nFinally, let’s map the data! We’ll begin by filtering out crashes missing location data, or with location (0,0):\n\ncrash_map_dat &lt;- filter(crash_dat, Latitude != 0, Longitude != 0)\n\nThe easiest way to check for outliers is simply to plot the data:\n\nggplot(crash_map_dat, aes(x=Longitude, y=Latitude)) + \n  geom_point(size=0.05, alpha = 0.5) # Small/transparent to handle overplotting\n\n\n\n\n\n\n\nFigure 7: Crash locations\n\n\n\n\n\nThe projection is a little wonky, but we can see the map taking shape. There are enough data points in our data set to make individual streets!\n\nIndividual crashes\nWe can improve our map by incorporating actual map data for New York City. Fortunately, most of the heavy lifting has already been done for us by the nycgeo package! Among other things, this package can split the geography up according to different types of administrative boundaries, from boroughs all the way down to invidual census tracts.\n\n# Load NYC map data (https://nycgeo.mattherman.info/index.html)\nmap_dat &lt;- nyc_boundaries(geography = \"tract\") # Split into census tracts\n\n# Add simple features (sf) to our data set\ncrash_map_dat &lt;- crash_map_dat %&gt;% \n  st_as_sf(coords = c(\"Longitude\",\"Latitude\"), \n           crs=4326,\n           stringsAsFactors = FALSE)\n\nWe can now overlay individual crashes as points on a map of census tracts:\n\nggplot(data = crash_map_dat) + \n  geom_sf(data = map_dat, mapping = NULL) +\n  geom_sf(size = 0.025, alpha = 0.25, color = \"red\") + \n  theme_void()\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n\nFigure 8: Crash locations\n\n\n\n\n\nPlotting crashes as individual points is useful for identifying where crashes occurred, but things get muddled by the sheer number of data points, especially where the data get dense (e.g., basically all of Manhattan).\n\n\nChoropleth map\nThe solution is to aggregate the data by dividing the map into subsections and coloring them based on the number of crashes. We’ll define the subsections based on census tracts, thus making a choropleth map!\nWe’ll also need a way to identify which tract each crash occurred in, since our data only includes the lat/lon of the crash. Fortunately, nyc_point_poly() will do just this!\n\ncrash_map_dat_tract &lt;- nyc_point_poly(crash_map_dat, \"tract\") %&gt;% \n  st_set_geometry(NULL)\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\nTrasnsforming points to EPSG 2263\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\ncrashes_by_tract &lt;- crash_map_dat_tract %&gt;%\n  count(tract_id, name = \"Crashes\", sort = TRUE)\n\n\nhead(crashes_by_tract, n=10)\n\n\n\nTable 3: Crashes per tract\n\n\n\n\n  \n\n\n\n\n\n\n\nNow we’ll join this with our map data and plot:\n\nleft_join(map_dat,crashes_by_tract) %&gt;% \n  ggplot() + \n  geom_sf(aes(fill = Crashes)) +\n  scale_fill_viridis_c(option = \"A\") +\n  coord_sf() +\n  theme_void() + \n  theme(legend.position = c(0.2,0.815)) # Position legend in blank area of plot\n\nJoining with `by = join_by(tract_id)`\nold-style crs object detected; please recreate object with a recent\nsf::st_crs()\n\n\n\n\n\n\n\n\nFigure 9: Crashes by census tract\n\n\n\n\n\nWe can use a similar approach to plot other variables by tract. Let’s look at the total number of injuries reported per tract:\n\ninjuries_by_tract &lt;- crash_map_dat_tract %&gt;% \n  group_by(tract_id) %&gt;% \n  summarise(Injuries =sum(Number_of_persons_injured)) %&gt;% \n  ungroup()\n\nhead(injuries_by_tract, n=10)\n\n\n  \n\n\n\n\nleft_join(map_dat,injuries_by_tract) %&gt;% \n  ggplot() + \n  geom_sf(aes(fill = Injuries)) +\n  scale_fill_viridis_c(option = \"A\") +\n  coord_sf() +\n  theme_void() + \n  theme(legend.position = c(0.2,0.815)) # Position legend in blank area of plot\n\nJoining with `by = join_by(tract_id)`\nold-style crs object detected; please recreate object with a recent\nsf::st_crs()\n\n\n\n\n\n\n\n\nFigure 10: Injuries by census tract\n\n\n\n\n\n\n\nHeatmap\nFor comparison, we can also make a heatmap. Same basic idea as the choropleth map (i.e., colors represent number of crashes per area), but the areas are formed by dividing the geography up into regular polygons, rather than using real-world divisions like census tracts.\n\nggplot() + \n  geom_sf(data = map_dat, mapping = NULL) +\n  geom_hex(data = filter(crash_dat, Latitude != 0, Longitude != 0), \n           aes(x=Longitude,y=Latitude),\n           binwidth=0.005) +\n  coord_sf(crs = 4326) +\n  scale_fill_viridis_c(option = \"A\") +\n  theme_void()+\n  theme(legend.position = c(0.2,0.815))\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n\nFigure 11: Crashes by location (heatmap)\n\n\n\n\n\nPersonally, I like the choropleth map better; it’s easier to see how the distribution of crashes maps onto the actual geography of the city.\n\n\nCrashes by street and tract\nIn both of our choropleth maps, we can see a couple of tracts with high rates of crashes relative to the surrounding areas (e.g., the long tract between Brooklyn and Queens that seems to correspond to Flushing Meadows Corona Park). I’m guessing that some of these high rates of crashes may be due to freeways and expressways running through the tracts in question. Conveniently, the data set includes the names of the street on which each crash occurred! Let’s look at the street with the most crashes in the top 20 tracts by crashes:\n\ncrash_map_dat_tract %&gt;%\n  count(tract_id, On_street_name, name = \"Crashes\", sort = TRUE) %&gt;% \n  drop_na(On_street_name) %&gt;% \n  filter(tract_id %in% head(crashes_by_tract$tract_id,20)) %&gt;% # Top 20 tracts\n  slice_max(order_by = Crashes, n=1, by = tract_id) # Street w/ most crashes\n\n\n\nTable 4: Street with most crashes per tract\n\n\n\n\n  \n\n\n\n\n\n\nAs expected, in the tracts with the most crashes, the street with the most crashes tends to be a parkway or expressway.\nWhile we’re at it, let’s look at the top 50 streets for crashes citywide. We’ll use the full data set (crash_dat) to include cases where the street was recorded but the latitude/longitude were not.\n\ncrash_dat %&gt;% \n  drop_na(On_street_name) %&gt;% \n  mutate(On_street_name = str_to_title(On_street_name)) %&gt;% \n  count(On_street_name, sort = TRUE, name = \"Crashes\") %&gt;% \n  head(n=50)\n\n\n\nTable 5: Street with most crashes citywide"
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#footnotes",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#footnotes",
    "title": "NYC Crash Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn general, vroom::vroom() is much faster than read.csv(), but here we’re also limited by connection speed; it took a little over a minute to read the data in with this approach.↩︎\nThere are a number of approaches for dealing with very large (and larger-than-memory) data sets, which I hope to cover in future posts.↩︎\nObligatory reference to that one Joy Division album cover.↩︎"
>>>>>>> 5592f3f (Draft Intro to R workshop)
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html",
    "href": "posts/2024-11-25-ggplot-workshop/index.html",
    "title": "Data Visualization with ggplot2",
    "section": "",
    "text": "Today’s post is adapted from a workshop I ran as part of my stats fellowship in my PhD program. My goal with this post is to cover the basics of data visualization with ggplot2 (natch), but also to shed a little light on the reasoning behind effective data communication.\nIn this post, we’ll cover the basics of ggplot2, then explore how to plot distributions and summary statistics, combine different representations of data in a single plot, customize plot appearance, and export plots.\nggplot2 implements a “grammar of graphics” (Wilkinson, 2005), allowing for the specification of plots in terms of individual elements, and the iterative creation of plots (we can layer elements in one by one). At heart, plots in ggplot2 consist of data (variables) mapped to aesthetics (e.g., position, color, and shape of plot elements).\n\n\nWe’ll be working with a simulated data set today (data-viz.csv); you can download it here. All the packages we’ll need can be loaded via library(tidyverse):\n\n# Load packages\nlibrary(tidyverse) \n\n# Load data\nmain_dat &lt;- read_csv(\"data-viz.csv\") \n\n# Overview of data\nsummary(main_dat)\n\n       ID          Condition            Gender             Major          \n Min.   :  1.00   Length:300         Length:300         Length:300        \n 1st Qu.: 75.75   Class :character   Class :character   Class :character  \n Median :150.50   Mode  :character   Mode  :character   Mode  :character  \n Mean   :150.50                                                           \n 3rd Qu.:225.25                                                           \n Max.   :300.00                                                           \n      SES           Anxiety            RT       \n Min.   :1.000   Min.   : 6.00   Min.   :112.0  \n 1st Qu.:2.000   1st Qu.:13.00   1st Qu.:180.8  \n Median :3.000   Median :16.00   Median :201.0  \n Mean   :2.637   Mean   :15.71   Mean   :200.7  \n 3rd Qu.:3.000   3rd Qu.:18.00   3rd Qu.:222.0  \n Max.   :5.000   Max.   :24.00   Max.   :279.0  \n\nglimpse(main_dat)\n\nRows: 300\nColumns: 7\n$ ID        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ Condition &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", …\n$ Gender    &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"M…\n$ Major     &lt;chr&gt; \"Psyc\", \"Hist\", \"Psyc\", \"Chem\", \"Hist\", \"Hist\", \"Hist\", \"Psy…\n$ SES       &lt;dbl&gt; 4, 3, 4, 1, 1, 1, 3, 3, 2, 2, 3, 2, 1, 1, 3, 5, 3, 2, 2, 1, …\n$ Anxiety   &lt;dbl&gt; 17, 14, 16, 17, 17, 19, 19, 17, 19, 17, 20, 15, 19, 16, 15, …\n$ RT        &lt;dbl&gt; 213, 151, 189, 240, 195, 187, 237, 178, 217, 179, 220, 182, …\n\n\nThe data is from an imaginary psychological experiment; we have three categorical variables (Condition, Major, and Gender) and three continuous variables (SES [socio-economic status], Anxiety, and RT [reaction time]) for each of 300 imaginary participants, each with a unique identifier (ID)."
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#load-data-set",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#load-data-set",
    "title": "Data Visualization with ggplot2",
    "section": "",
    "text": "We’ll be working with a simulated data set today (data-viz.csv); you can download it here. All the packages we’ll need can be loaded via library(tidyverse):\n\n# Load packages\nlibrary(tidyverse) \n\n# Load data\nmain_dat &lt;- read_csv(\"data-viz.csv\") \n\n# Overview of data\nsummary(main_dat)\n\n       ID          Condition            Gender             Major          \n Min.   :  1.00   Length:300         Length:300         Length:300        \n 1st Qu.: 75.75   Class :character   Class :character   Class :character  \n Median :150.50   Mode  :character   Mode  :character   Mode  :character  \n Mean   :150.50                                                           \n 3rd Qu.:225.25                                                           \n Max.   :300.00                                                           \n      SES           Anxiety            RT       \n Min.   :1.000   Min.   : 6.00   Min.   :112.0  \n 1st Qu.:2.000   1st Qu.:13.00   1st Qu.:180.8  \n Median :3.000   Median :16.00   Median :201.0  \n Mean   :2.637   Mean   :15.71   Mean   :200.7  \n 3rd Qu.:3.000   3rd Qu.:18.00   3rd Qu.:222.0  \n Max.   :5.000   Max.   :24.00   Max.   :279.0  \n\nglimpse(main_dat)\n\nRows: 300\nColumns: 7\n$ ID        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ Condition &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", …\n$ Gender    &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"M…\n$ Major     &lt;chr&gt; \"Psyc\", \"Hist\", \"Psyc\", \"Chem\", \"Hist\", \"Hist\", \"Hist\", \"Psy…\n$ SES       &lt;dbl&gt; 4, 3, 4, 1, 1, 1, 3, 3, 2, 2, 3, 2, 1, 1, 3, 5, 3, 2, 2, 1, …\n$ Anxiety   &lt;dbl&gt; 17, 14, 16, 17, 17, 19, 19, 17, 19, 17, 20, 15, 19, 16, 15, …\n$ RT        &lt;dbl&gt; 213, 151, 189, 240, 195, 187, 237, 178, 217, 179, 220, 182, …\n\n\nThe data is from an imaginary psychological experiment; we have three categorical variables (Condition, Major, and Gender) and three continuous variables (SES [socio-economic status], Anxiety, and RT [reaction time]) for each of 300 imaginary participants, each with a unique identifier (ID)."
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#adding-plot-elements",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#adding-plot-elements",
    "title": "Data Visualization with ggplot2",
    "section": "Adding plot elements",
    "text": "Adding plot elements\nHaving mapped variables onto the x-axis and y-axis, we can create a scatterplot by adding points via geom_point():\n\n# We'll omit argument names (data, mapping) going forward\n# ggplot() assumes the first argument is data and the second is mapping\nggplot(main_dat, aes(x = Anxiety, y= RT)) + geom_point() \n\n\n\n\n\n\n\n\ngeom_point() is an example of a geom, a geometric representation of data. We’ll see a number of different geoms later!\nNext, we can set the color of the points to be based on Condition by mapping it onto color in the original aes():\n\nggplot(main_dat, aes(x = Anxiety, y = RT, color = Condition)) + \n  geom_point()\n\n\n\n\n\n\n\n\nWe can map Major to the shape of the points in the same way:\n\nggplot(main_dat, aes(x = Anxiety, y = RT, color = Condition, shape = Major)) + \n  geom_point()\n\n\n\n\n\n\n\n\nEach addition to the aesthetic specification maps a new variable onto a new aesthetic of the plot. Also notice that we now get two legends, one for color and one for shape."
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#saving-plots",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#saving-plots",
    "title": "Data Visualization with ggplot2",
    "section": "Saving plots",
    "text": "Saving plots\nTo save1 a plot for later use (and save ourselves a lot of copy-pasting), we can assign ggplot() output to an object:\n\nRT_by_anxiety_cond &lt;- ggplot(main_dat, aes(x = Anxiety, y = RT, \n                                           color = Condition, shape = Major)) + \n  geom_point()\n\nCalling the object then displays the plot:\n\nRT_by_anxiety_cond"
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#faceting",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#faceting",
    "title": "Data Visualization with ggplot2",
    "section": "Faceting",
    "text": "Faceting\nWe now have four variables (Anxiety, RT, Condition, and Major) in one plot – very efficient, but a little hard to read. One solution is to split the plot into facets, giving us a mini-plot for each level of a factor…\n\nRT_by_anxiety_cond + facet_wrap(~Major) # \"By major\"\n\n\n\n\n\n\n\n\n…or for combinations of levels of factors:\n\nRT_by_anxiety_cond + facet_grid(Major~Condition) # \"By major and condition\"\n\n\n\n\n\n\n\n\nSwitch order of variables in facet_grid() to flip arrangement of facets:\n\nRT_by_anxiety_cond + facet_grid(Condition~Major) \n\n\n\n\n\n\n\n\nNotice that all of the facets have shared (aligned) axes, allowing us to compare data (e.g., point positions) between facets!\nfacet_grid() always creates rows and columns based on the faceting variable(s) you specify, which makes it useful when you want to facet by two specific variables. In contrast, facet_wrap() will wrap the facets into rows and columns, which makes it useful when you want to facet by a single variable that has more than three or so values:\n\n# New plot for this example\nggplot(main_dat, aes(x=Anxiety,y=RT)) + geom_point() + facet_wrap(~SES)\n\n\n\n\n\n\n\n\nNotice how SES levels 4 and 5 wrap around to make a second row!"
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#coordinates",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#coordinates",
    "title": "Data Visualization with ggplot2",
    "section": "Coordinates",
    "text": "Coordinates\nWe can also customize the coordinate system used by our plot. Let’s force our y-axis includes zero:\n\n# To keep an auto-calculated limit, use NA\nRT_by_anxiety_cond + coord_cartesian(ylim = c(0,NA)) \n\n\n\n\n\n\n\n\nWe can swap X and Y axes using coord_flip():\n\nRT_by_anxiety_cond + coord_flip(ylim = c(0,NA)) # y-axis is now horizontal!\n\n\n\n\n\n\n\n\nOther useful coordinate systems:\n\ncoord_fixed() fixes aspect ratio of plot (good if X and Y are in same units)\ncoord_map()projects portion of globe onto 2D plane (good for maps!)"
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#theme",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#theme",
    "title": "Data Visualization with ggplot2",
    "section": "Theme",
    "text": "Theme\nThemes allow us to change the appearance of the plot as a whole:\n\nRT_by_anxiety_cond + theme_light() # Use light background\n\n\n\n\n\n\n\nRT_by_anxiety_cond + theme_minimal() # Omit bounding boxes\n\n\n\n\n\n\n\nRT_by_anxiety_cond + theme_classic() # More traditional style\n\n\n\n\n\n\n\n\nWe can tweak the font2 used in the plot via arguments to the theme function:\n\nRT_by_anxiety_cond + theme_classic(base_size = 9, base_family = \"serif\") \n\n\n\n\n\n\n\n\nWe can manually customize further by adding an additional theme() function:\n\nRT_by_anxiety_cond + \n  theme(legend.position = \"bottom\") # Move legend to bottom\n\n\n\n\n\n\n\nRT_by_anxiety_cond + \n  facet_grid(Condition~Major) + \n  theme(legend.position = \"none\") # Remove unnecessary legends\n\n\n\n\n\n\n\n\nVirtually every aspect of the plot’s appearance can be customized via the theme() function, from major/minor axes to tick marks to the axis titles."
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#recap",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#recap",
    "title": "Data Visualization with ggplot2",
    "section": "Recap",
    "text": "Recap\nLet’s pause and review how to construct a plot with ggplot():\n\nWe start by giving ggplot() a data frame (here, main_dat)\nWithin the aes() argument, we map variables to specific elements (x and y position, color, and shape)\nWe provide (at least one) geom to represent the data (here, geom_point() to get points)\nWe can facet by one (or more) variables to produce multiple plots with shared axes\nWe can specify coordinate systems (e.g., coord_flip())\nThemes let us customize overall plot appearance\n\nNext, we’ll explore how we can use these tools to visualize different types of data!"
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#histograms",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#histograms",
    "title": "Data Visualization with ggplot2",
    "section": "Histograms",
    "text": "Histograms\nWe can make histograms with geom_histogram(). We don’t map anything to the y-axis – R automatically maps frequency.\n\nggplot(main_dat, aes(x = Anxiety)) + # No Y aesthetic!\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can manually set size of bins:\n\nggplot(main_dat, aes(x = Anxiety)) + \n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\nWhat if we want to look at the distribution separately for each level of Condition? We can map Condition onto the fill color of the histogram via the fill aesthetic3:\n\nggplot(main_dat, aes(x = Anxiety, fill = Condition)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\nThis plot’s not great – it’s hard to read counts for Control and Treatment 1. Instead, let’s try faceting:\n\nggplot(main_dat, aes(x = Anxiety, fill = Condition)) + \n  geom_histogram(binwidth = 2) + \n  facet_wrap(~Condition, ncol = 1) + # Force facets into single column\n  theme(legend.position = \"none\") # Remove superfluous legend\n\n\n\n\n\n\n\n\nThis plot makes it much easier to compare distributions between the three groups – notice that x-axis (Anxiety) scores are aligned across all three facets."
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#bar-plot-of-frequencies",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#bar-plot-of-frequencies",
    "title": "Data Visualization with ggplot2",
    "section": "Bar plot of frequencies",
    "text": "Bar plot of frequencies\nFor categorical variables (like Condition), we can get frequency as a bar plot using geom_bar(). Just like with histograms, we don’t map anything to the y-axis; the geom automatically calculates the height of the bars (in this case, by counting the number of cases).\n\nggplot(main_dat,aes(x = Gender)) + # No Y aesthetic!\n  geom_bar()"
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#background",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#background",
    "title": "Data Visualization with ggplot2",
    "section": "Background",
    "text": "Background\nUnder the hood, all geometric elements have a stat that determines how the data translates into the properties of the geom:\n\ngeom_point(): x and y coordinates of points come directly from numeric values (identity)\ngeom_histogram(): height of bin comes from number of values in bin (count)\ngeom_bar(): height of bar comes from number of values in category (count)\n\ngeom_point() creates one object for each data point, while geom_histogram() and geom_bar() use a built-in summary statistic. Sometimes, we’ll want to tweak this process (e.g., if we want to plot a different summary statistic):\n\nBarplot: height of bar from the mean of each group\nPointrange: height of point from mean of group, width of range from variability of group\n\nThere are two possible approaches here:\n\nCalculate summary statistics for each group, then plot these values\n\ndata %&gt;% group_by() %&gt;% summarize() %&gt;% ggplot()\n\nHave ggplot generate specific summary statistics for us\n\ndata %&gt;% ggplot() + stat_summary()\n\n\nToday, we’ll focus on the second approach!"
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#getting-summary-statistics-on-the-fly",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#getting-summary-statistics-on-the-fly",
    "title": "Data Visualization with ggplot2",
    "section": "Getting summary statistics on the fly",
    "text": "Getting summary statistics on the fly\nLet’s get a bar plot in which the bar heights represent group means (rather than frequencies):\n\nggplot(main_dat, aes(x = Condition, y = Anxiety)) + \n  stat_summary(fun = \"mean\", geom = \"bar\") # Get the mean, represent via bars\n\n\n\n\n\n\n\n\nIf we want to get fancy, we can specify summary functions that return multiple values for each group, and geoms that use all these values:\n\nggplot(main_dat, aes(x = Condition, y = Anxiety)) + \n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"pointrange\")\n\n\n\n\n\n\n\n\nWe use fun.data because the function we’ve specified (mean_cl_normal) returns a data frame consisting of the mean as well as the upper and lower bounds of the 95% confidence interval."
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#multiple-2-geoms",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#multiple-2-geoms",
    "title": "Data Visualization with ggplot2",
    "section": "2 Multiple 2 Geoms",
    "text": "2 Multiple 2 Geoms\nWe can have more than one geom in the same plot! Let’s add a line:\n\nggplot(main_dat, aes(x = Condition, y = Anxiety)) + \n  # Plot means as line, all one group\n  stat_summary(fun = \"mean\", geom = \"line\", group = 1) +\n  # Plot means/CIs as pointranges\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"pointrange\")\n\n\n\n\n\n\n\n\nNotice the line uses fun (instead of fun.data) because it represents a single summary statistic (the mean).\nWe can combine summary statistics (our pointrange and lines) with raw data (i.e., individual data points):\n\nggplot(main_dat, aes(x = Condition, y = Anxiety)) + \n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"pointrange\") + \n  stat_summary(fun = \"mean\", geom = \"line\", group = 1) +\n  geom_point() # Add in raw data\n\n\n\n\n\n\n\n\nTo avoid overplotting (data points directly on top of each other), we can make the points partially transparent:\n\nggplot(main_dat, aes(x = Condition, y = Anxiety)) + \n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"pointrange\") + \n  stat_summary(fun = \"mean\", geom = \"line\", group = 1) + \n  geom_point(alpha = .1) # 1 = opaque, 0 = fully transparent\n\n\n\n\n\n\n\n\nWe can also use a different geom to summarize the data:\n\nggplot(main_dat, aes(x = Condition, y = Anxiety)) + \n  geom_violin() + # Put first so pointranges/lines are drawn on top\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"pointrange\") + \n  stat_summary(fun = \"mean\", geom = \"line\", group = 1)"
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#multiple-pointranges-multiple-lines",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#multiple-pointranges-multiple-lines",
    "title": "Data Visualization with ggplot2",
    "section": "Multiple pointranges, multiple lines",
    "text": "Multiple pointranges, multiple lines\nLet’s break things down by major as well as condition!\n\nmain_dat %&gt;% \n  ggplot(aes(x = Condition, y = Anxiety, \n             color = Major, group = Major)) + # Specify how to group!\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"pointrange\") + \n  stat_summary(fun = \"mean\", geom = \"line\") + \n  geom_point(alpha = .1)\n\n\n\n\n\n\n\n\nTo make things easier to read, let’s dodge the geoms for each group (i.e., shift them slightly left/right so they’re not stacked atop each other):\n\nmain_dat %&gt;% \n  ggplot(aes(x = Condition, y = Anxiety, color = Major, group = Major)) + \n  stat_summary(fun.data = \"mean_cl_normal\", \n               geom = \"pointrange\", \n               position = position_dodge(width = .5)) + \n  stat_summary(fun = \"mean\", \n               geom = \"line\",\n               position = position_dodge(width = .5)) +\n  geom_point(alpha = .1, position = position_dodge(width = .5))\n\n\n\n\n\n\n\n\nNote that we need to specify dodge for each geom to keep things aligned!\nLet’s save this plot for later use:\n\nAnxiety_by_cond_major &lt;- main_dat %&gt;% \n  ggplot(aes(x = Condition, y = Anxiety, color = Major, group = Major)) + \n  stat_summary(fun.data = \"mean_cl_normal\", \n               geom = \"pointrange\", \n               position = position_dodge(width = .5)) + \n  stat_summary(fun = \"mean\", \n               geom = \"line\",\n               position = position_dodge(width = .5)) +\n  geom_point(alpha = .1, position = position_dodge(width = .5))\n\nNotice how we were able to build this (fairly complex) plot incrementally: we started by mapping Condition and Anxiety to the x and y axes, selected appropriate geoms to display our data (both summary statistics via stat_summary() and raw values via geom_point()), added another variable (Major) mapped onto color, and finally tweaked the geoms’ position to avoid overplotting. This process aligns nicely with how I tend to approach data visualization: I start by thinking about what variables I want to plot, then figure out how to display them via reckless experimentation."
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#changing-labels",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#changing-labels",
    "title": "Data Visualization with ggplot2",
    "section": "Changing labels",
    "text": "Changing labels\nWe can set labels within scale_*() functions, or by using the convenience function labs(). Let’s try this with our earlier scatterplot:\n\nRT_by_anxiety_cond + # Back to our scatterplot!\n  labs(title = \"Mean reaction time by anxiety score, condition, and major\", \n       subtitle = \"Check out my awesome subtitle!\",\n       caption = \"Can note exclusion criteria, define error bars, etc.\",\n       x = \"Anxiety score\", # List of scales and names\n       y = \"Mean reaction time (ms)\")\n\n\n\n\n\n\n\n\nNote that all of the labels display within the plot area, and are distinct from figure captions that may be generated by (e.g.) RMarkdown or Quarto. In some contexts, it may be helpful to put as much information as possible in the plot itself (e.g., for a presentation or conference poster), while in other contexts this information may be displayed in the figure caption instead (e.g., a journal article). Here’s what this same figure might look like if we took the second approach:\n\nRT_by_anxiety_cond + \n  labs(x = \"Anxiety score\", # List of scales and names\n       y = \"Mean reaction time (ms)\")\n\n\n\n\n\n\n\nFigure 1: Mean reaction time by anxiety score, condition, and major. Check out my awesome subtitle! Here’s some information about exclusion criteria, error bars, etc."
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#customizing-continuous-axes",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#customizing-continuous-axes",
    "title": "Data Visualization with ggplot2",
    "section": "Customizing continuous axes",
    "text": "Customizing continuous axes\nWe can add arguments to scale_*() functions to customize the upper/lower bounds, and to determine where the breaks fall:\n\nRT_by_anxiety_cond +\n  scale_y_continuous(name = \"Mean reaction time (ms)\", # Another way to label!\n                     limits = c(0,300), # Upper and lower bounds\n                     breaks = 0:6*50)  # Should give vector of break points\n\n\n\n\n\n\n\n\nIf your limits clip some of your data, you’ll get a warning:\n\nRT_by_anxiety_cond +\n  scale_y_continuous(name = \"Mean reaction time (ms)\",\n                     limits = c(0,200), # Clips off upper end of data range\n                     breaks = 0:6*50)\n\nWarning: Removed 151 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIf you are calculating summary statistics via stat_summary(), data that gets clipped off won’t be included in summary statistics, which can skew results. It’s a good idea to set the “window” via coord instead:\n\nRT_by_anxiety_cond + \n  coord_cartesian(ylim = c(0,200))"
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#customizing-discrete-axes",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#customizing-discrete-axes",
    "title": "Data Visualization with ggplot2",
    "section": "Customizing discrete axes",
    "text": "Customizing discrete axes\nWe can manually relabel levels of our discrete (categorical) x-axis. Let’s try on our pointrange plot:\n\nAnxiety_by_cond_major + # Our pointrange plot\n  scale_x_discrete(labels = c(\"0mg\", \"500mg\", \"1000mg\")) # Applied in order\n\n\n\n\n\n\n\n\nIn general, to relabel levels, tweak the scale that produced them. Here, for our scatterplot, we’ll relabel Condition (mapped onto color) and Major (mapped onto shape):\n\nRT_by_anxiety_cond + # Our scatterplot\n  scale_color_discrete(labels = c(\"0mg\", \"500mg\", \"1000mg\")) +\n  scale_shape_discrete(labels = c(\"Chemistry\",\"History\",\"Psychology\"))"
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#adjusting-colors",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#adjusting-colors",
    "title": "Data Visualization with ggplot2",
    "section": "Adjusting colors",
    "text": "Adjusting colors\nWe can change the palettes R uses for colors. Let’s practice with a brand-new plot:\n\n# Create bar plot of gender frequency\nGender_plot &lt;- ggplot(main_dat, aes(x = Gender, fill = Gender)) + \n  geom_bar()\n\n# Default palette\nGender_plot\n\n\n\n\n\n\n\n# Greyscale\nGender_plot + scale_fill_grey()\n\n\n\n\n\n\n\n# Brewer\nGender_plot + scale_fill_brewer(type = \"qual\", # Qualitative palettes\n                                palette = \"Set2\") # Specific palette\n\n\n\n\n\n\n\n\nWe can also manually specify colors via scale_fill_manual(), either by name:\n\nGender_plot + \n  scale_fill_manual(values =c(\"darkorange\", \"dodgerblue\", \"green\", \"purple\"))\n\n\n\n\n\n\n\n\nOr by hex code (useful to match existing palettes such as school or corporate color schemes):\n\nGender_plot +\n  scale_fill_manual(values =c(\"#003660\", \"#FEBC11\", \"#043927\", \"#c4b581\"))\n\n\n\n\n\n\n\n\nFor ordinal variables (like SES), might make sense to use a sequential palette:\n\nggplot(main_dat, aes(x = SES, fill = factor(SES))) + # Make R treat as factor\n  geom_bar() +\n  scale_fill_brewer(type = \"seq\") # Sequential palette\n\n\n\n\n\n\n\n\nIf a factor is ordered, ggplot automatically uses scale_viridis (an excellent sequential palette):\n\n# Distribution of SES\nggplot(main_dat, aes(x = SES, fill = ordered(SES))) + \n  geom_bar()\n\n\n\n\n\n\n\n# Distribution of SES by condition\nggplot(main_dat, aes(x = Condition, fill = ordered(SES))) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNotice that for all of these examples except the last, color is just for visual appeal; it doesn’t add information to the plot beyond what’s already communicated by the bars themselves. When color does convey information, it’s important to consider accessibility (e.g., whether the plot can still convey information effectively to viewers with colorblindness). Some options include using another aesthetic as a fallback (e.g., mapping the variable onto shape as well as color), faceting the plot, using a colorblind-friendly palette (such as scale_viridis)4."
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#more-ggplot2-resources",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#more-ggplot2-resources",
    "title": "Data Visualization with ggplot2",
    "section": "More ggplot2 resources",
    "text": "More ggplot2 resources\n\nOfficial ggplot2 reference: First stop for specifics of ggplot2 functions!\nR Graphics Cookbook (2e): Great for anytime you ask yourself “How the heck do I do X”?\nggplot2: elegant graphics for data analysis: For when you’re ready to go deeper!\nFundamentals of Data Visualization: For big-picture questions about data visualization and communication best practices!"
  },
  {
    "objectID": "posts/2024-11-25-ggplot-workshop/index.html#footnotes",
    "href": "posts/2024-11-25-ggplot-workshop/index.html#footnotes",
    "title": "Data Visualization with ggplot2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Save” here means “keep in memory”, not “save to disk”; we’ll cover that a little later.↩︎\nUsing custom fonts in ggplot2 is tricky, to say the least. The three options guaranteed to work on any graphic device are sans (the default), serif, and mono; anything else is a roll of the dice. I hope to cover this topic in a little more detail in a future post, since I’ve spent a good amount of time tearing my hair out over it.↩︎\nFor geoms that don’t have an “interior” (e.g., points and lines), the color aesthetic sets the color of the whole geom. For geoms that do have an interior (e.g., histograms and bars), the fill aesthetic sets the interior color and the color aesthetic sets the outline color. It can be a little tricky to remember – if specifying one aesthetic doesn’t work the way you expect, try specifying the other and see if that has the desired effect.↩︎\nDisclaimer: I am not an expert in accessibility, and cannot guarantee that these tips will make plots fully or even partially compliant with regulation regarding accessibility (e.g., the ADA). Please don’t sue me!↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an unapologetic data nerd with a passion for turning messy data into understandable, actionable insights. I have tackled projects ranging from investigations of the cognitive mechanisms of ingroup bias, to explorations of gender differences in science education programs for elementary schools, to assessments of learning outcomes in leadership development workshops for state employees. I have substantial experience with framing research questions, developing and implementing appropriate methods (experimental and/or survey-based designs), assessing and aligning existing data sources, conducting statistical analyses, and communicating findings to stakeholders via reports, dashboards, and more."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\nMy skills include:\n\nExperimental design\nSurvey development\nData aggregation, cleaning, and analysis\nData visualization and communication\nReproducible analysis and reporting workflows\nTeaching/tutoring statistical methods and research best practices\nR + tidyverse + Quarto, Git + GitHub, SQL, Python, SPSS, LaTeX, Qualtrics"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nUniversity of California, Santa Barbara\nPh.D. in Psychological and Brain Sciences, June 2022\n\nEmphasis in Quantitative Methods in Social Science (QMSS)\nCertificate in College and University Teaching (CCUT)\n\n\n\nCalifornia State University, Sacramento\nM.A. in Psychology, May 2016\n\n\nOccidental College\nB.A. in Psychology, May 2011\n\nMinor in Mathematics"
  }
]