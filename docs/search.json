[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "NYC Crash Data\n\n\n\n\n\n\nR\n\n\nData Cleaning\n\n\nData Visualization\n\n\nMapping\n\n\n\nExploring a data set of motor vehicle crashes in New York City\n\n\n\n\n\nMay 6, 2024\n\n\nJack Strelich\n\n\n\n\n\n\n\n\n\n\n\n\nTPBP\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\n\nTest Post Best Post\n\n\n\n\n\nFeb 7, 2024\n\n\nJack Strelich\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "I’m Jack Strelich, and this is my website! I blog about data wrangling, analysis, and visualization."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Welcome",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nNYC Crash Data\n\n\nExploring a data set of motor vehicle crashes in New York City\n\n\n\nJack Strelich\n\n\nMay 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTPBP\n\n\nTest Post Best Post\n\n\n\nJack Strelich\n\n\nFeb 7, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n See all"
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html",
    "href": "posts/2024-05-04-NYC-Crashes/index.html",
    "title": "NYC Crash Data",
    "section": "",
    "text": "In this post, I’ll explore a data set of motor vehicle crashes in New York City obtained via data.gov. This data set interested me for two reasons. First, it’s quite comprehensive, covering all crashes resulting in injury and/or over $1000 in damage since mid-2012. Second, it’s messy in a way that’s very representative of real-world data (and very hard to replicate with more “shrink-wrapped” data sets).\nMy goals for this analysis were to perform some basic data cleaning, get some insights from the data, and practice a type of visualization I don’t get to use very often in my day job: the choropleth map!\nTo start, we’ll load the packages we need:\n\n# Load packages\n## Handling dates\nlibrary(lubridate)\n\n## Mapping\nlibrary(sf)\nlibrary(nycgeo)\n\n## And starring...\nlibrary(tidyverse)\n\n# Set theme for plots\ntheme_set(theme_minimal())\n\nNow we’ll read in the data. Because the CSV is so large (~437 Mb), I decided to read it directly via the URL, rather than downloading and reading the file1. Once the data is loaded, we’ll tweak the column names for readability (converting to sentence case and replacing spaces with underscores), read the Crash_date column via lubridate::as_date(), and change the names of the columns containing vehicle type codes to be consistent with other column names. We’ll also filter the data to look only at crashes in 2023, to avoid using too much memory2.\n\n# Load crash data (https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv)\ncrash_url &lt;- \"https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv\"\n\n# Load data (takes a minute!)\ncrash_dat &lt;- vroom::vroom(file = crash_url, # Read from URL\n                          guess_max = 10^3,\n                          .name_repair = ~str_replace_all(str_to_sentence(.x), \n                                                          pattern = \" \", \n                                                          replacement = \"_\")) %&gt;%  \n  mutate(Crash_date= as_date(Crash_date, format = \"%m/%d/%Y\")) %&gt;% \n  filter(year(Crash_date) == 2023) %&gt;% # Only 2023 data\n  rename_with(\\(var) str_replace_all(var, \n                                     pattern = \"Vehicle_type_code\", \n                                     replacement = \"Type_code_vehicle\"))\n\nRows: 2084770 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (16): Crash_date, Borough, Location, On_street_name, Cross_street_name,...\ndbl  (12): Zip_code, Latitude, Longitude, Number_of_persons_injured, Number_...\ntime  (1): Crash_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#time-and-date",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#time-and-date",
    "title": "NYC Crash Data",
    "section": "Time and date",
    "text": "Time and date\nFirst, let’s look at incidents by month:\n\nggplot(crash_dat, aes(x = month(Crash_date, label = TRUE))) + \n  geom_bar() +\n  labs(x = \"Month\", y = \"Incidents\")\n\n\n\n\n\n\n\nFigure 1: Incidents by date\n\n\n\n\n\nNo obvious differences by month/season! Next, let’s consider crash times. Looking at raw crash time data, there seems to be an unusually high number of crashes at exactly midnight (00:00):\n\ncount(crash_dat, Crash_time,sort = TRUE) %&gt;% head(n=10)\n\n\n  \n\n\n\nIn fact, we see almost twice as many crashes reported at midnight as at 5pm! This makes me suspect that in at least some cases a crash time of 00:00 represents missing data; for the purposes of this graph, we’ll exclude them.\nNow to see whether the time that incidents occurred varied by month. To help us compare months, we’ll use a ridgeline plot3, via ggridges::geom_density_ridges():\n\ncrash_dat %&gt;% \n  filter(Crash_time &gt; 0) %&gt;% \n  mutate(Crash_month = month(Crash_date, label = TRUE)) %&gt;% \n  ggplot(aes(x=Crash_time, \n             color=Crash_month,\n             fill=Crash_month,\n             y = fct_rev(Crash_month))) + \n  ggridges::geom_density_ridges(alpha = 0.4,\n                                quantile_lines = TRUE,\n                                quantiles = 2) +\n  scale_color_viridis_d() +\n  scale_x_time(breaks = (0:6)*14400) +\n  labs(x = \"Time of day\", y = \"Month\", \n       caption = \"Vertical lines represent median.\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 2: Incidents by month and time of day\n\n\n\n\n\nCrashes by time of day look fairly consistent across months; notice that the median of each distribution is around 2pm. Additionally, we can see peaks in the number of incidents between 8-9am and between 5-6pm (representing rush hour, I’m assuming)."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#injuries-and-fatalities",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#injuries-and-fatalities",
    "title": "NYC Crash Data",
    "section": "Injuries and fatalities",
    "text": "Injuries and fatalities\nTo explore crash outcomes, we’ll use pivot_longer() to make a data frame with multiple rows per crash, each row representing the number of motorists, cyclists, or pedestrians (Type) who were injured or killed (Outcome) in that crash.\n\noutcomes_by_crash &lt;- crash_dat %&gt;% \n  select(Collision_id, \n         starts_with(\"Crash\"), # Keep date and time\n         starts_with(\"Number_of_\")) %&gt;%\n  pivot_longer(cols = starts_with(\"Number_of_\"), \n               names_prefix = \"Number_of_\", # Clean up col names\n               names_sep = \"\\\\_\", # Split at underscore\n               names_to = c(\"Type\",\"Outcome\")) %&gt;% \n  mutate(Crash_month = month(Crash_date, label = TRUE),\n         Type = str_to_title(Type),\n         Outcome = str_to_title(Outcome))\n\n# Check our work\nhead(outcomes_by_crash, n = 10)\n\n\n  \n\n\n\nNow we can plot total injuries/fatalities for motorists, cyclists, and pedestrians by month. Because fatalities are (thankfully) far rarer than injuries, we’ll use the scales = \"free_y\" argument to facet_wrap() to let the two halves of the plot use different y-axis limits. We’ll also use fct_reorder() to make sure the ordering of categories in the legend matches the ordering of the categories in the graph itself.\n\noutcomes_by_crash %&gt;% \n  filter(Type != \"Persons\") %&gt;% \n  mutate(Type = fct_reorder(Type, value, sum, .desc = T)) %&gt;% \n  ggplot(aes(x = Crash_month, \n             y = value, \n             color = Type,\n             group = Type)) + \n  stat_summary(geom=\"line\", fun = \"sum\") +\n  stat_summary(geom=\"point\", fun = \"sum\") + \n  facet_wrap(~Outcome,\n             scales = \"free_y\",\n             nrow = 2) + \n  labs(y = \"Number of persons\", x= \"Month\")\n\n\n\n\n\n\n\nFigure 3: Injuries and fatalities by month\n\n\n\n\n\nInterestingly, injuries and fatalities for motorists seem to be highest in summer, while injuries and fatalities for pedestrians are lower during these months."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#vehicles",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#vehicles",
    "title": "NYC Crash Data",
    "section": "Vehicles",
    "text": "Vehicles\nNext, let’s explore the contributing factors and types of vehicles involved.\n\nCleaning\nWe’ll restructure the data such that each row represents one vehicle, rather than one incident!\n\nvehicle_dat &lt;- crash_dat %&gt;% \n  select(Collision_id, starts_with(c(\"Contributing_factor\",\"Type_code\"))) %&gt;% \n  pivot_longer(-Collision_id, \n               names_sep = \"_vehicle_\", \n               names_to = c(\".value\",\"Vehicle\")) %&gt;% \n  drop_na() %&gt;% \n  mutate(Vehicle = as.factor(as.numeric(Vehicle)))\n\nNow we can look at the different vehicle type codes and contributing factors that occur in the data:\n\ncount(vehicle_dat, Type_code, sort = TRUE)\ncount(vehicle_dat, Contributing_factor, sort = TRUE)\n\n\n\nTable 1: Incidents by vehicle type and contributing factor (raw)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\nThere are a lot of categories in here, and many of them appear to overlap (e.g., “Motorcycle” and “Motorbike”). To clean up a bit, we’ll change all Type_code entries to sentence case, then manually consolidate some levels:\n\nvehicle_dat &lt;- vehicle_dat %&gt;% \n  mutate(Type_code = str_to_sentence(Type_code),\n         Type_code = case_when( # checks a series of conditionals\n           str_detect(Type_code, \n                      coll(\"wagon\", ignore_case = TRUE)) ~ \"SUV/station wagon\",\n           str_detect(Type_code, \n                      coll(\"sedan\",ignore_case = TRUE)) ~ \"Sedan\",\n           .default = Type_code),\n         Type_code = case_match( # Replaces (vectors of) matches, OLD ~ NEW\n           Type_code,\n           \"Bicycle\" ~ \"Bike\",\n           \"Motorbike\" ~ \"Motorcycle\",\n           c(\"Ambul\",\"Ambu\",\"Amb\") ~ \"Ambulance\",\n           c(\"Unkno\",\"Unk\") ~ \"Unknown\",\n           c(\"Fire\",\"Fdny\",\"Firetruck\",\n             \"Firet\",\"Fdny truck\",\"Fdny fire\") ~ \"Fire truck\",\n           \"E-sco\" ~ \"E-scooter\",\n           \"E-bik\" ~ \"E-bike\",\n           .default = Type_code) %&gt;% \n           fct_lump_prop(0.005), # Lump codes occurring less than 0.5% \n         Contributing_factor = fct_lump_prop(\n           str_to_sentence(Contributing_factor), \n           0.005)) # same for contributing factor\n\nNow let’s check our work:\n\n(crashes_by_type &lt;- count(vehicle_dat, Type_code, \n                          sort = TRUE, name = \"Crashes\"))\n(crashes_by_factor &lt;- count(vehicle_dat, Contributing_factor, \n                            sort = TRUE, name = \"Crashes\"))\n\n\n\nTable 2: Incidents by vehicle type and contributing factor\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\nIt’s not perfect, and a subject matter expert could probably identify more types/factors that could be combined, but it should be workable for our purposes.\n\n\nExploring\n\nWhat types of vehicles were involved in crashes?\n\nggplot(crashes_by_type, aes(x = fct_rev(fct_inorder(Type_code)), y = Crashes)) + \n  geom_col() + coord_flip() +\n  labs(x = \"Vehicle type\")\n\n\n\n\n\n\n\nFigure 4: Incidents by vehicle type\n\n\n\n\n\nWithout knowing more about how common different types of vehicles are in NYC, we can’t make strong inferences from this data; i.e., the prevalence of sedans, SUVs, and station wagons in the crash data likely reflects the prevalence of these vehicles on the road, period. If we wanted to explore whether certain vehicle types are disproportionately likely to be involved in crashes, we’d need to know not just how many of each vehicle type are present in the city, but also how many hours each vehicle type is in motion on average (since, for example, buses and box trucks probably spend more of their time driving than private passenger vehicles, even if there are more of the latter).\n\n\nWhat contributing factors were involved in crashes?\n\ncrashes_by_factor %&gt;% \n  filter(Contributing_factor != \"Unspecified\") %&gt;% # Ignore missing data\n  ggplot(aes(x = fct_rev(fct_inorder(Contributing_factor)), y = Crashes)) + \n  geom_col() + coord_flip() +\n  labs(x = \"Contributing factor\")\n\n\n\n\n\n\n\nFigure 5: Incidents by contributing factor\n\n\n\n\n\nHere, we can draw more inferences from the data alone. Distracted driving seems to be a clear issue, as well as following too closely and failing to yield.\n\n\nHow many vehicles were involved per crash?\nWe can also examine how many vehicles were involved per crash, and plot the distribution:\n\ncount(vehicle_dat, Collision_id, name = \"Vehicles\") %&gt;% # Count vehicles per collision\n  count(Vehicles, name = \"Crashes\") %&gt;% # Count collisions for each number of vehicles\n  mutate(Proportion = Crashes/sum(Crashes),\n         Label = paste0(scales::percent(Proportion),\"\\n(n=\",Crashes,\")\")) %&gt;% \n  ggplot(aes(x = Vehicles, y = Crashes, label = Label)) + \n  geom_col() + labs(x = \"Number of vehicles involved\") +\n  geom_text(nudge_y = 3000, size = 3)\n\n\n\n\n\n\n\nFigure 6: Incidents by number of vehicles involved\n\n\n\n\n\nPerhaps unsurprisingly, the majority of crashes involved two vehicles; crashes involving three or more vehicles were relatively rare (&lt;10% of crashes)."
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#mapping",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#mapping",
    "title": "NYC Crash Data",
    "section": "Mapping",
    "text": "Mapping\nFinally, let’s map the data! We’ll begin by filtering out crashes missing location data, or with location (0,0):\n\ncrash_map_dat &lt;- filter(crash_dat, Latitude != 0, Longitude != 0)\n\nThe easiest way to check for outliers is simply to plot the data:\n\nggplot(crash_map_dat, aes(x=Longitude, y=Latitude)) + \n  geom_point(size=0.05, alpha = 0.5) # Small/transparent to handle overplotting\n\n\n\n\n\n\n\nFigure 7: Crash locations\n\n\n\n\n\nThe projection is a little wonky, but we can see the map taking shape. There are enough data points in our data set to make individual streets!\n\nIndividual crashes\nWe can improve our map by incorporating actual map data for New York City. Fortunately, most of the heavy lifting has already been done for us by the nycgeo package! Among other things, this package can split the geography up according to different types of administrative boundaries, from boroughs all the way down to invidual census tracts.\n\n# Load NYC map data (https://nycgeo.mattherman.info/index.html)\nmap_dat &lt;- nyc_boundaries(geography = \"tract\") # Split into census tracts\n\n# Add simple features (sf) to our data set\ncrash_map_dat &lt;- crash_map_dat %&gt;% \n  st_as_sf(coords = c(\"Longitude\",\"Latitude\"), \n           crs=4326,\n           stringsAsFactors = FALSE)\n\nWe can now overlay individual crashes as points on a map of census tracts:\n\nggplot(data = crash_map_dat) + \n  geom_sf(data = map_dat, mapping = NULL) +\n  geom_sf(size = 0.025, alpha = 0.25, color = \"red\") + \n  theme_void()\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n\nFigure 8: Crash locations\n\n\n\n\n\nPlotting crashes as individual points is useful for identifying where crashes occurred, but things get muddled by the sheer number of data points, especially where the data get dense (e.g., basically all of Manhattan).\n\n\nChoropleth map\nThe solution is to aggregate the data by dividing the map into subsections and coloring them based on the number of crashes. We’ll define the subsections based on census tracts, thus making a choropleth map!\nWe’ll also need a way to identify which tract each crash occurred in, since our data only includes the lat/lon of the crash. Fortunately, nyc_point_poly() will do just this!\n\ncrash_map_dat_tract &lt;- nyc_point_poly(crash_map_dat, \"tract\") %&gt;% \n  st_set_geometry(NULL)\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\nTrasnsforming points to EPSG 2263\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\ncrashes_by_tract &lt;- crash_map_dat_tract %&gt;%\n  count(tract_id, name = \"Crashes\", sort = TRUE)\n\n\nhead(crashes_by_tract, n=10)\n\n\n\nTable 3: Crashes per tract\n\n\n\n\n  \n\n\n\n\n\n\n\nNow we’ll join this with our map data and plot:\n\nleft_join(map_dat,crashes_by_tract) %&gt;% \n  ggplot() + \n  geom_sf(aes(fill = Crashes)) +\n  scale_fill_viridis_c(option = \"A\") +\n  coord_sf() +\n  theme_void() + \n  theme(legend.position = c(0.2,0.815)) # Position legend in blank area of plot\n\nJoining with `by = join_by(tract_id)`\nold-style crs object detected; please recreate object with a recent\nsf::st_crs()\n\n\n\n\n\n\n\n\nFigure 9: Crashes by census tract\n\n\n\n\n\nWe can use a similar approach to plot other variables by tract. Let’s look at the total number of injuries reported per tract:\n\ninjuries_by_tract &lt;- crash_map_dat_tract %&gt;% \n  group_by(tract_id) %&gt;% \n  summarise(Injuries =sum(Number_of_persons_injured)) %&gt;% \n  ungroup()\n\nhead(injuries_by_tract, n=10)\n\n\n  \n\n\n\n\nleft_join(map_dat,injuries_by_tract) %&gt;% \n  ggplot() + \n  geom_sf(aes(fill = Injuries)) +\n  scale_fill_viridis_c(option = \"A\") +\n  coord_sf() +\n  theme_void() + \n  theme(legend.position = c(0.2,0.815)) # Position legend in blank area of plot\n\nJoining with `by = join_by(tract_id)`\nold-style crs object detected; please recreate object with a recent\nsf::st_crs()\n\n\n\n\n\n\n\n\nFigure 10: Injuries by census tract\n\n\n\n\n\n\n\nHeatmap\nFor comparison, we can also make a heatmap. Same basic idea as the choropleth map (i.e., colors represent number of crashes per area), but the areas are formed by dividing the geography up into regular polygons, rather than using real-world divisions like census tracts.\n\nggplot() + \n  geom_sf(data = map_dat, mapping = NULL) +\n  geom_hex(data = filter(crash_dat, Latitude != 0, Longitude != 0), \n           aes(x=Longitude,y=Latitude),\n           binwidth=0.005) +\n  coord_sf(crs = 4326) +\n  scale_fill_viridis_c(option = \"A\") +\n  theme_void()+\n  theme(legend.position = c(0.2,0.815))\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n\nFigure 11: Crashes by location (heatmap)\n\n\n\n\n\nPersonally, I like the choropleth map better; it’s easier to see how the distribution of crashes maps onto the actual geography of the city.\n\n\nCrashes by street and tract\nIn both of our choropleth maps, we can see a couple of tracts with high rates of crashes relative to the surrounding areas (e.g., the long tract between Brooklyn and Queens that seems to correspond to Flushing Meadows Corona Park). I’m guessing that some of these high rates of crashes may be due to freeways and expressways running through the tracts in question. Conveniently, the data set includes the names of the street on which each crash occurred! Let’s look at the street with the most crashes in the top 20 tracts by crashes:\n\ncrash_map_dat_tract %&gt;%\n  count(tract_id, On_street_name, name = \"Crashes\", sort = TRUE) %&gt;% \n  drop_na(On_street_name) %&gt;% \n  filter(tract_id %in% head(crashes_by_tract$tract_id,20)) %&gt;% # Top 20 tracts\n  slice_max(order_by = Crashes, n=1, by = tract_id) # Street w/ most crashes\n\n\n\nTable 4: Street with most crashes per tract\n\n\n\n\n  \n\n\n\n\n\n\nAs expected, in the tracts with the most crashes, the street with the most crashes tends to be a parkway or expressway.\nWhile we’re at it, let’s look at the top 50 streets for crashes citywide. We’ll use the full data set (crash_dat) to include cases where the street was recorded but the latitude/longitude were not.\n\ncrash_dat %&gt;% \n  drop_na(On_street_name) %&gt;% \n  mutate(On_street_name = str_to_title(On_street_name)) %&gt;% \n  count(On_street_name, sort = TRUE, name = \"Crashes\") %&gt;% \n  head(n=50)\n\n\n\nTable 5: Street with most crashes citywide"
  },
  {
    "objectID": "posts/2024-05-04-NYC-Crashes/index.html#footnotes",
    "href": "posts/2024-05-04-NYC-Crashes/index.html#footnotes",
    "title": "NYC Crash Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn general, vroom::vroom() is much faster than read.csv(), but here we’re also limited by connection speed; it took a little over a minute to read the data in with this approach.↩︎\nThere are a number of approaches for dealing with very large (and larger-than-memory) data sets, which I hope to cover in future posts.↩︎\nObligatory reference to that one Joy Division album cover.↩︎"
  },
  {
    "objectID": "posts/2024-02-07-TEST/index.html",
    "href": "posts/2024-02-07-TEST/index.html",
    "title": "TPBP",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Praesent elementum facilisis leo vel fringilla est ullamcorper eget nulla. Enim nulla aliquet porttitor lacus luctus. Ligula ullamcorper malesuada proin libero nunc. In metus vulputate eu scelerisque felis imperdiet proin fermentum. Vivamus arcu felis bibendum ut tristique et egestas. Pellentesque elit ullamcorper dignissim cras tincidunt lobortis feugiat. Nunc non blandit massa enim nec. Quis hendrerit dolor magna eget est lorem ipsum dolor. Eu non diam phasellus vestibulum lorem sed risus ultricies. Sagittis vitae et leo duis ut diam quam. Gravida cum sociis natoque penatibus et magnis dis parturient.\n\nlibrary(ggplot2)\nggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() + theme_minimal()\n\n\n\n\nMPG by HP\n\n\n\n\nElit scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique. Diam vulputate ut pharetra sit amet aliquam id diam. Sagittis purus sit amet volutpat. Faucibus vitae aliquet nec ullamcorper sit amet risus nullam eget. Nulla facilisi morbi tempus iaculis urna. Viverra adipiscing at in tellus integer feugiat scelerisque varius morbi. Mollis aliquam ut porttitor leo a diam. Vitae et leo duis ut. Natoque penatibus et magnis dis parturient montes nascetur. Nisl rhoncus mattis rhoncus urna neque viverra justo nec.\n\n# Let's check the distro!\nstem(mtcars$mpg)\n\n\n  The decimal point is at the |\n\n  10 | 44\n  12 | 3\n  14 | 3702258\n  16 | 438\n  18 | 17227\n  20 | 00445\n  22 | 88\n  24 | 4\n  26 | 03\n  28 | \n  30 | 44\n  32 | 49\n\n\nAuctor neque vitae tempus quam pellentesque. Porttitor massa id neque aliquam vestibulum morbi blandit cursus risus. Id ornare arcu odio ut sem nulla pharetra. Volutpat commodo sed egestas egestas. Tellus in hac habitasse platea dictumst. Blandit aliquam etiam erat velit scelerisque. Etiam dignissim diam quis enim lobortis. Pellentesque id nibh tortor id. Amet justo donec enim diam vulputate ut pharetra sit amet. Mattis vulputate enim nulla aliquet porttitor lacus luctus accumsan tortor. Massa massa ultricies mi quis hendrerit. Vitae proin sagittis nisl rhoncus mattis rhoncus urna. Porttitor rhoncus dolor purus non enim praesent elementum facilisis. Dui sapien eget mi proin sed libero enim sed. A diam maecenas sed enim ut sem viverra. Nibh tortor id aliquet lectus. Faucibus scelerisque eleifend donec pretium vulputate sapien nec sagittis. Interdum velit laoreet id donec ultrices tincidunt."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an unapologetic data nerd with a passion for turning messy data into understandable, actionable insights. I have tackled projects ranging from investigations of the cognitive mechanisms of ingroup bias, to explorations of gender differences in science education programs for elementary schools, to assessments of learning outcomes in leadership development workshops for state employees. I have substantial experience with framing research questions, developing and implementing appropriate methods (experimental and/or survey-based designs), assessing and aligning existing data sources, conducting statistical analyses, and communicating findings to stakeholders via reports, dashboards, and more."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\nMy skills include:\n\nExperimental design\nSurvey development\nData aggregation, cleaning, and analysis\nData visualization and communication\nReproducible analysis and reporting workflows\nR + tidyverse + Quarto, Git + GitHub, SQL, Python, SPSS, LaTeX, Qualtrics"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nUniversity of California, Santa Barbara\nPh.D. in Psychological and Brain Sciences, June 2022\n\nEmphasis in Quantitative Methods in Social Science (QMSS)\nCertificate in College and University Teaching (CCUT)\n\n\n\nCalifornia State University, Sacramento\nM.A. in Psychology, May 2016\n\n\nOccidental College\nB.A. in Psychology, May 2011\n\nMinor in Mathematics"
  }
]